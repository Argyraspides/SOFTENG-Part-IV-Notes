Uh, yeah. Generally. Right. So you didn't think that would be one of the main things going on here, right.  But also, there's a lot I mean, especially this year, uh, at Chi, I was there too.  I it's like  to % of research, I swear to God is I, I mean, it's everywhere now.  Um, but that's not a bad thing, you know? And it is a tool for HCI.  So, yeah. Welcome to advanced, uh, HCI class. Um, today is just sort of the basic introduction.  Um, so just sort of sit back and relax, try to understand what's going on in the realm of HCI.  There are a lot of tools that we use here or that you can use and that you definitely we're not expecting.  Uh, there are projects here which you may not have thought it would be like from the beginning before you sort of signed up for this course.  But they're all very interesting, I can assure you that, uh, the way how today is going to be structured is basically three main content.  The first is the general introduction to the course.  So I'll just like to introduce all the instructors and uh, also sort of the overview of the course.  I mean, a lot of it is already documented in your canvas, but it's just more digestible here.  So I'm going to go through, uh, what uh, the sort of each week you are going to be working on.  And then I'm going to talk about introduction to HCI in general.  And uh, I also prepared a reading material for you. You can find it on canvas as well.  So the content is based on that, plus a lot of examples that I'm also preparing for you, I have prepared for you.  And lastly is the research projects. And the third point research projects is the key differentiating factor with the HCI.  Uh, the other HCI class. So this advanced HCI.  And basically the main point here is that you guys will need to be working as a group, uh, in projects, HCI based projects.  All right. So I will spend or we will spend the last, uh, hour or so to focus on the discussions on the research projects.  What are the research projects, how does it and how does the scoring work and so on, so forth.  All right. So I'm going to introduce the instructors first.  We have a total of four instructors lecturers for this course.  Uh well not us four but uh the other half is away.  So I'll have to take take over, uh, to introduce them.  So the first is, uh, Robert Elmore. Uh, he is a professor in computer science.  Uh, co-founder of CSS. I'm actually not sure what that is, but sounds like a big deal.  And he is, uh, he's been around for a while, actually, and his main, uh, sort of field is in construction, construction and also RV.  So, uh, an example of a work that he did before is that's very HCI is gesture based navigation for VR buildings.  And, uh, this is actually if you work with a part four student, . students, uh, and the idea,  if I'm not mistaken, is how to navigate virtual buildings, purely relying on gestures.  So you see, that's forward backward, rotate right. Rotate left purely with hand gestures.  You get to navigate to full of virtual space.  And even at this point you can start thinking, what are the tools required to build a prototype like this?  What are the knowledge required? Right.  You you need to have, uh, maybe some computer vision, maybe some, um, D design, uh, abilities, uh, VR development,  programming, even machine learning, let's say in AI, machine learning and AI, because you do recognise the gestures.  So there's a lot of sort of discipline required for you to build a prototype like this.  Our second slide, actually, she's the, uh, coordinator.  Course coordinator. So this is Danielle. Uh, and she's got a long history, so let's see.  She did her bachelors in HCI at Toronto, also PhD in Toronto.  And then she was a postdoc at Stanford.  Then for a good time, I believe she was a sort of, uh, UX researcher, like a senior UX researcher at Yahoo for quite a lot of years.  I think maybe some of them know no more than I do. And now she's, uh, she was a, uh, sort of lecturer senior, actually.  And finally now she's associate professor, also in computer science.  So she's a course coordinator. So if there's anything that's unclear of she would know the best.  But of course, uh, we have four instructors so you can sort of UX anyone.  Um, and the sort of main content that she was sharing is like, is actually an app that she's helped design previously as a video chat app.  Uh, she says it's her UX baby. Uh, it's called Cabana.  I'm not sure if it's the only app, so maybe you guys can look it up. Maybe not.  I'm not sure. So she's a is a project from a Yahoo days?  Um, several years, several years ago.  At least it's a video chat app. And our third instructor is Tracy.  And Tracy's here. So maybe you can introduce yourself.  It's a microphone. I have a microphone for you guys.  Then I'll go to, um, Carla Carter.  Uh, no, no, uh uh, wannabe me, uh, new Hakka, uh, rookie, um, Caucasian and able to talk her in.  Um, so my name is Tracy Porter. Um, my phone.  I come from, uh, away on, uh, the west coast and east coast of the North Island.  Um, so I am Maori, Scots and Austrian, um, with, uh, my slide.  Um, so I my background is in, um, I have a bachelor of it, a master's of information studies,  and, um, just about finished my PhD, which is also in Maori studies.  Um, previously I've worked as a lecturer. Um, I'm from poly, uh, .  Um, Victoria University of Wellington. I'm teaching in the School of Information, uh management's, uh, master's, uh, paper,  and I'm currently a professional teaching fellow, um, here at the university.  Um, so I'm going to be taking classes and weeks five and six, and my background is in the glam sector, so galleries, libraries, archives and museums.  And I'll be sharing, um, resources with you for your literature review.  And also in, uh, week six, I'll be talking to you about bi cultural frameworks,  um, and Maori design principles when it comes to HCI, human computer interaction.  So yeah. So that's me. So you'll see me from time to time in the lectures.  Oh, um, yeah. And this is a slide of my research.  Um, so, uh, my current PhD research is looking at do Maori, um, revitalisation in libraries, archives and museums.  Um, so these cultural heritage institutes are the largest, uh, holders of town Maori.  So, um, uh, Maori treasures, uh, and, um, it's looking at what they're doing to, uh,  share that mattered and to share that knowledge, um, that they hold with the community.  So, yeah. That's me. Kyoto. All right.  Thank you. Tracy. So. And the last one is myself.  So my name is Eun Spent. Pi. You just call me Pi. Um, I'm actually from Malaysia.  And Malaysians here. And said, all right.  So I did my degree in master's in mechanical engineering, actually, but I found it to be boring and I went to its HCI.  So I did my PhD in Japan, Keio University. The school is called School of Media Design.  Um, is a post-grad school actually. So I did my Ph.D. there.  Uh, the sequence is not exactly right because I did, I did, and then I came to ABA in Oakland to do my post-doc for about one year and ten months.  And then I was back to Keio as an assistant professor for about two and a half years.  Now I'm back to yoga, but this time in computer science as a lecturer.  So I'm just sort of alternating between Japan and New Zealand.  I suppose. People keep asking me why I come here, and I came from Japan.  Uh, it's a good question, actually. So, yeah, um, for me, I have several research interests, but I have a main sort of research vision,  uh, which is to design and develop an inclusive reality,  which is, uh, leveraging the sense of reality to bridge gaps among people towards an inclusive, pro-social space.  So this is my general research vision, and I explore a couple of things to make this a reality.  And I like to sort of separate this into three main pillars, which is, uh, cognition assistance and augmentation.  Uh, I don't want to get too much in depth to any of them because they may take a while.  And also for the lecture today, uh, for introduction to HCI,  I have actually included some videos of my past research as an example to understand it a little bit better.  So that's for later. This for now, this is what I sort of explore the general umbrella.  So cognition is about understanding people assistants how to, you know,  provide assistance to those with any kinds of needs, either disability or any kinds of diverse,  uh, users, neurodiverse and augmentation, which is pushing beyond how can we augment human ability to perform beyond their natural limits.  And I look at this as general idea of an inclusive reality.  Okay. So that's about it for the for instructors.  Now I will go towards the course overview.  Like I said the sort of main content is in the canvas.  So after the lecture you can also have a look at the canvas, read it up and it'll describe properly the full content of the course.  Uh, advanced HCI focuses heavily on the research methods.  So you will learn evaluation methods, analytical tools and also synthesising results.  And as I mentioned earlier, the most important point is the research project, which is working as a team.  You have a research output from the stage of the literature review until the sort of final prototyping stage and,  uh, video production stage as well, actually. Okay.  All right. So let's go through a little of the course content for each week so you know what to expect afterwards.  So this is the week one. Uh like I said earlier is the course introduction understanding HCI and the general Research Project introduction.  Uh, and um, this is kind of the general layout with the three hour lecture I know is really long, but for three hour lectures,  basically the first two is on the lecture itself, and the final hour is on the discussions on the research projects with the tutors.  Okay. Um, so you'll find that the second point, usually the project ups.  So in week one we'll introduce the projects. And then by the end of this class you can already start discussing who are your potential team-mates.  And one of the projects that you're interested in. I'll describe the process afterwards.  And in week two you can uh, we will. Uh, well, it's still me, so I'm coming the first few weeks.  Week two is on multimodal interfaces. So I will then introduce to you guys what are the different tools that you can use in HCI.  Different input mechanics, different interfaces it should be aware of.  And your project stand up. So project stand up basically means sort of like a discussion stage strike the third hour.  And for the project stand up, um, for the second week is basically just catch up with the tutors again,  try to if there's any questions that you have regarding any of the projects, that would be a good time.  You can still update your sort of, uh, member lists or the, uh, select the projects that you're interested in.  Okay. Uh, week three is on experiment design.  So the lecture will cover general experiment design as well as quantitative analysis.  Uh, and very basic quantitative analysis. Right. So there are a few methods for this week.  Three we'll look at how to, uh, use some kind of basic statistical tools to analyse results.  Uh, and the projects stand up will also be finalising your research groups.  So by week three, if anyone is not in any group they will be assigned a group.  So a week three sort of the final point you have the deadline of getting your project,  uh, members and your project title already selected, project ready.  And then week four, uh, I believe Robert will be covering this. It's on intersection design and Fitts law.  If you already know what is law, that's good.  If you don't, week four will be covering is basically one of the fundamental, you know, uh, rules to learn in HCI.  And also here you'll be discussing a little bit about domain specific HCI.  And then the project stand out will discuss more detail about how you plan to approach each project.  So the agenda, the final I will would be discussions with the tutors.  The tutors will be here in the third hour and you get to catch them, discuss with them,  and they will be helping you as much as they can with your research project. Week five, uh, is on literature review and it'll be a test.  So week  or  will be covered by Tracy. And we'll sort of understand how literature review is done the correct way.  And then it'll be sort of like a myth. Some tests by week five. Um, this shouldn't be any stand up by then.  And week five test is like a separate slot.  Uh, week six will cover case studies, so she will share some case studies, peer review process, and also the, uh, stand up.  So in this case, the stand up will be just sort of general project feedback, understanding the progress,  how you've been going so far with your team-mates, uh, any concerns you can share with the tutors?  They'll try their best to assist you again. All right. Then after week six, I believe all this should belong to Danielle already.  So week seven, epistemology and a literature review workshop.  If you don't know what epistemology is, then that's where you'll learn.  Do me a while to pronounce that correctly.  And then there is more Literary Review workshop and the stand up, which will focus on the implementation stage of the research plan.  Okay. So you will be discussing on how to actually implement the prototype, uh, whatever it is required for the research project for week eight.  Uh, it focuses on ethics and qualitative research. So you recall previously I say quantitative this time is qualitative research.  So we'll talk about ethics.  What you should consider, uh, when designing a prototype or designing an experiment as well as the qualitative analysis methods,  which is, which is actually very, very different from quantitative.  If you guys have, you know, if you have any idea about that, um, just stand up will be more on implementation.  So in this case there's uh, some demo discussions on demo and implementation stage again.  Uh, week nine is systems week, so there's no lecture then. And uh, week ten is on protocols and implementation do.  So the lecture here will be discussing about protocols implementation.  But uh, you know in the stand up itself is also on that feedback.  So feedback on protocol and implementation. Um yeah.  And then the final two weeks, week  and , week  will focus on the pilot studies.  If you need to do some kind of data collection. So there's no lecture there as well.  And week  is the final week. Whereas the final presentation, video presentation, final report also a general reflection on the research projects.  And you know, the final stand out, which is the feedback on the videos and overall findings of the research project.  Okay. So there is a heavy emphasis on research, uh, projects here.  I as I mentioned earlier, every week we try to have a proper discussion slot for this.  So, you know, you know, you can progress properly. So in the course.  Uh, so this is what everyone's also interested in, right? The grading grading process.  So the mid-semester test will be % literature review, % research projects, %, final exam, %, reflection % and peer review %.  And the research project itself is then broken down. So there's a broken down part of the % of the research project.  Um, so the components for completion, these are points that you shouldn't need to meet the requirements, which you will know in time.  Uh, before they can be awarded, uh, such as submitting the topic, planning the video, implementing stand up and the PSA bonus points.  So the PSA bonus point is basically something that, um, is too, so is recommended in Piazza.  We get bombarded by questions, and a lot of them is also already covered in lectures which which makes it,  you know, extra challenging for us to also go through each of the questions and answer them one by one.  And we know we really covered them.  So Piazza bonus points actually refers to like, uh, if the points is already something that was mentioned before in a lecture on the slide somewhere.  So asking them again, asking them again on Piazza won't do you any good.  So you need to find it out first by yourself or help your peers.  So if someone if you know the question to it, look at Piazza, help your peers and we'll give you a bonus point on that.  Yeah. So this is %, but something, uh, just so that we can sort of, you know, be able to manage the Q&A in Piazza a little bit better.  Um, so you don't think, like, the moment is something you don't know?  The first thing you do is ask. The question is, that's not really how it how it works.  You should do some research to see what has been covered, or maybe discuss with your peers.  Then you can, you know, if you really need to. Then of course you can post on Piazza and could try our best to help you in the case.  And then there's the components part, which is the % final video and % final report, uh, which is sort of like the main submission by week .  Okay. So it's okay if some of these are not so clear yet.  You know, in time, as you understand what the research projects are, as you know, what you need to do, then it gets more and more clear.  Um, also actually yesterday, well, last night I just uploaded these slides.  Uh, the the right partner wasn't there yet. I just updated it this morning to include a bit more information.  So if you downloaded the slides last night, yeah, it wouldn't have this much information.  So I would recommend if you haven't done so just download this morning then you'll get this information in.  Okay, so that's mainly what I would like to cover for the sort of general understanding of  what the course is and what to expect from the course over the next couple of months.  Yeah. Okay, so after this, we will move on to the, uh, lecture introduction to HCI.  Okay. So the main reading material here is research methods in HCI and human computer interaction.  Uh, specifically, I picked up chapter one, uh, introduction to a Shared Research, page  to .  This book you can find on the library online. You have to go to the physical library.  You go to the link. Uh, such a library. Search up the book title.  Search the chapter, and then you'll find it. Yeah. Uh.  Or otherwise, the link is provided. The reading material link link is provided for you on the canvas page, and it's quite a good book.  I actually referred to this book a few times, and I think it appears again as a reading material for another, uh, upcoming lecture,  but at least for the introduction, I think it's very good to understand more a little bit about what it's about.  Maybe the first video that I showed you guys can be a little bit overwhelming, right?  There's a lot of things that you didn't expect it would be part of. HCI doesn't mean that you will need to pick up all of the skills.  Not at all. Okay. It depends on your research project.  In the end. Okay.  So how did HCI come to be? Right. It's sort of an interesting field, isn't it?  It's not like physics or biology over like computer science.  HCI has become some sort of like a subfield within computer science.  I would say computer science is the main pillar of HCI, but, um, it can get a bit complicated and tricky.  So let's just quickly rewind a little bit and see how it came to be.  Uh, and at least from the first video, you get to understand AI is a multidisciplinary field, right?  There is some hardware, there's some software, there's some design, there's some CAD modelling, there's some making food.  Right. Making robots here and there. So it's actually hard to pinpoint, but I could say that the general consensus would be around the s,  specifically in , where the first Chi conference was out.  Is that, uh, is interesting actually, they you if you look up the actual research publications,  you can find it, uh, the, the very first, uh, publication of Chi Chi , uh, which is at, uh, the US.  Um, yeah. However, it's not to say that it's, it's the very, very start of actual HCI practice because actual practice was way before that.  It's just only by around  when the conference came up, that everyone can sort of unanimously agreed, okay.  He's. Yeah, this is called HCI. So I'll give you some examples too, on how it was like prior to , right before the Big Bang.  So we'll see what it's like. So this this guy Ivan Sutherland, very famous.  Anyone know of Ivan if you did some computer science history research.  No. Okay. Well, Ivan Sutherland in , this is actually his PhD research.  He demonstrated a device that allows direct manipulation of virtual objects.  And it kind of looks like that. What does it what does it look like to everyone?  Anyone know what look can you draw comparison to? Oh, what is he even doing?  Anybody. Well, it looks like he's sketching, right?  Like sketching on a screen, which is something very normal today on your phones and your iPads.  So this could be one of the very first devices that does so.  And it was created in . It's called the sketchpad, which is basically the iPad of the s.  Um, yeah. This is a video. I'll show you how that looks on the computer to draw a line.  It was all live from this position where I am now, in any subsequent position of my life, that this is much like a rubber band stuck in two pens.  We take it for granted. These things by the s is basically a breakthrough, isn't it?  This. Anyway, I want to know. I lost track in there.  I've used a pen to fast, and that's all the computer's got.  Drawing a line. Well, if you notice that white dot the jump on to the line as I get close to the dot in the centre of the cross.  When you get close to, I jump over onto it. When you do that, it's much like a gravity field at the endpoint,  and there's even a higher gravity field to allow us to position the point exactly online, or in this case, exactly the end point.  This allows me to move my pen by crossing a stop line and line, and get a position going up at the same time.  So now I'm going to draw a second line. And even the third one.  Now I don't earn it. Uh, we probably don't have to watch the whole thing, but look at that, right?  In the s, he had a stylus. You see, when he moves away from the display.  Stop striking. When he's close to the line, it snaps to the line.  Radio position. Can you imagine these things prior to iPad?  Right. You can't. So this is in the s again. And it's really quite amazing.  You can do basic geometry here right? You could erase you can draw circles.  I could go back with you erase it and I can line it up the other way.  Uh, the computer has and it could do quite sort of complex D geometry.  Right? Like the lines. So that's that's quite something.  It's called a sketch pad. And, uh, you know, in computer science, it's a very famous sort of output.  Um, that really pioneered a lot of what we do today.  So, yeah, all the all the devices in front of you in your pockets. Uh, it's easy to take it for granted.  But a lot of these things, when they first came out, I believe also a lot of people don't feel I made much sense.  But in time, it just proves itself to be sort of it changed the landscape of how computing works.  Like, even for me, um, I remember when I transitioned to a touchscreen phone,  I was telling myself, there's no way I'm going to use the touchscreen phone. I'm just going to use, you know, I want tactile, physical buttons.  These things don't make sense. And, you know, I clearly I was wrong.  So, you know, I, you know everyone uses touchscreens.  One I'm doing the same. And so it's it's difficult to envision the future of how like when he came up with this,  I'm pretty sure he wasn't thinking it would transform the landscape of computing. And it's very niche as well.  It's cool. Right. But how much it will progress in the future is hard to say.  But look at where we are today. At least small.  Then let's move to another very famous guy, uh, Douglas Engelbart.   well, actually, it's a little bit earlier, but this is when he demoed what he designed .  He demonstrated a device that would change computing as we know it.  Any idea? Well, you seen this slide.  So you may have known. Yes, exactly. So he designed the mouse is one of the first.  And the mouse this is not something that you want to take for granted as well, isn't it?  It's an input. This. This is his mouse. When he first designed it and when he demonstrated it, it was often called the mother of all demos.  So even people not in the tech field, they sort of some of them at least knew about it when he first demonstrated it.  So that's how the first sort of mouse look like. And even until today, you can see the resemblance.  Right. There is a button there. So I mean, the tracking methods are different obviously, but you can clearly tell that it has evolved from here.  And here was sort of the beginning point of, of the design. And it paved the way for general computing, uh, all the way to today.  Right. We are still using mouses, mice, mice, mouse mice, uh, until now.  So it's not something that has uh, that has changed very dramatically.  Now let's go to the third invention. This one's my favourite.  So this is also around s and s is like well, it's like an amazing like a renaissance period for computer science, I feel.  So Ivan again, same guy Ivan Sutherland's clearly he was not happy with sketchpad.  You know, anybody could do it. So he then invented this next, next invention, this thing.  What? What is this? Well, what do you guys think this is? Or have you seen this before?  Yeah. We are. Looks like we got, doesn't it? It's basically the very first AR VR device mounted on the ceiling like that.  And then you sort of put your head there. In fact, this exact prototype of his is in demonstration.  It's in the museum Computer Science Museum in Mountain View.  So if you ever find yourself in the US Mountain View, head to the Computer Science Museum and you'll find this exact device there.  So yeah, VR since the s he has been around.  And how does it look like? Let me show you. In fact, it's not even purely VR, right?  It's actually R, and R is something that is also it's only in the last few years, you know,  with the Vision Pro and Quest two, quest three that things have progressed dramatically.  But in the s, I even designed this AR headset where you can see a virtual cube floating in midair.  That changes its perspective depending on your position, right?  So it gives the feeling that it's really there, which was in in the physical virtual object in physical space.  And that's really quite something. Uh, this prototype is called the Sword of Damocles.  Um, well, because if you look at the picture before, it's basically it's very heavy.  So it's not something that you can just when you're in your face, it's not it's a head mounted display,  but it's not wearable is mounted on the ceiling with this giant cross.  That's why it was sort of called the Sword of Damocles. See if you know Greek legend.  Well, I'm not too familiar with it, but the story of Damocles is about some sword dangling above his head.  So that's where the the the name came from. It became known as the Sword of Damocles.  And in these videos you can find on YouTube. So you just look up the keywords.  So Damocles first VR headset, you'll find this video by, um, Ivan Sutherland.  So the s is, I will say, amazing period for HCI to.  And the time I didn't know what HCI was.  They just know that okay, I want to make something that users can intuitively interact with the virtual environment with computers,  uh, in ways that can push the boundaries of computing as opposed to just entering command lines at that time.  If you do move the command lines on you on your screens.  And so these explorations, you know, uh, way before, way before, uh, the Chi conference was already being explored.  And we acknowledge these are some of the sort of the fundamental pioneers, uh, in this field.  And then it became a little bit more practical as well.  So, um, the Social Security Administration, uh, in the US in , they also ran a lot of sort of design process.  They do things like task analysis, scenario generation and prototyping, usability testing.  And people came to know that this process is basically kind of HCI, where you do some data collection.  You you run some prototype, you try to evaluate its usability.  So that's actually basically HCI, not so much in the fancy prototypes like the VR headset and the mouse,  which is, you know, fancy at the time, but so more, more so on the process.  Right? HCI is also the process.  And there are different kinds of contributions for HCI, which is something that I will talk about a little bit later on.  Um, yeah. And then basically from the s to early s, uh, computers generally moved from research labs to your homes and offices.  So, you know, your parents may have some knowledge of some of these devices.  Uh, and then many realised that the intersection of with computers is now becoming more and more prevalent, more and more important.  And everyone is using it, not just scientists and labs, which used to be scientists in labs.  So then, you know, computing sort of evolved. The left is the Apple tool, middle is an IBM machine.  The third is the it was a Commodore Commodore machine, which I think is really interesting.  The third one where the whole PC is actually integrated keyboard.  You don't really see that very often, do you? Um, but yeah, these are, these are some of the, uh, evolution of computing.  Obviously there's a lot more.  I don't plan to fill the slides with computer pictures and explain each model, but at least the important ones are some of these, right.  Apple IBM, Commodore. And then where we are today, isn't it small computers in front of you?  They can do everything that you need. Right.  So that would be sort of the general high level take on the history of HCI and how we progress to the point we are today.  Um, so then what are the types of HCI contributions?  There are obviously a lot of different kinds of contributions because of how multidisciplinary it is.  There are ways of which you can contribute to HCI beyond just making a new input device.  You know, that is very common. You do that a lot new input, new interaction, but there are a lot of ways where you can also contribute HCI.  And there are I'm going to go through each of them, uh, so that you can understand the landscape of how HCI research is performed.  The first main kind of contribution is called the empirical contribution.  And this is sort of one of the main ways of contributing HCI.  Uh, and you also see this contribution overlapping with other contributions.  Okay. So this is the main one where you collect data either quantitative, qualitative or mixed way.  And you go through methods like general experiment design surveys, focus groups,  you know, workshops, time diaries, logging of the sensor values, ethnographic studies.  So in a general sense, this is what we call empirical contribution, where you report on the data based on a certain experiment design.  And you would say that the main goal is to reveal insights of human behaviour in relation to computing,  because of course, empirical contribution applies to a lot of kinds of research fields.  Every research field out there, you know, you need to collect data in and run experiments.  Report is true. But what makes it HCI is that it's insights of human behaviour in relation to computing.  Because HCI is human, computer interaction is about understanding human factors and human behaviour.  And that's actually one of the sort of key points, uh, about this.  This is one of the two main contributions. The second main contribution is an artefact contribution and this is where the cool stuff comes in.  It is the design and development of novel artefacts like an interface, toolkit, architecture, etc. uh,  and often we use empirical data or empirical, you know, contribution site to also validate them.  In most cases they do fall into three main categories, which is a new system, a new technique, or a new design.  They should all show new possible futures and implications.  So right, if you look at the VR headset, look at the mouse that Ivan Sutherland and Douglas Engelbart and even the videos that you watched earlier,  they are meant to show you new possible futures and implications.  And in such a step that for regular people who are in the field,  it would look like a gimmick or even something that what what's anyone going to do with this?  That's that's what people in the field would feel like.  But you in an HCI research field, you're supposed to look at it and think, what are the potential implications that it can be used for in the future?  Right. Who would've thought it would become the next future VR headset?  Who would have thought it would be the computing device as a mouse to work with every computers out there?  So you need to think about the applications.  You think about what you can apply for the different fields, you know, can it be worked in medicine field?  Can you use it to play a game, any of this.  So you try to think about the possible futures and implication. When you think about a when you design a new artefact.  So these are two main contributions that are a lot more, a few more of which I'm going through,  which are admittedly less popular than empirical and artefact, but they still exist and are still very valid HCI research.  All right. The third point is methodological contribution, uh, where you propose a new method in process or practices.  For example, a new application modification. New metric of assessment.  Right. So is more on the method side as opposed to something that you can see your touch or feel.  Uh, methodological contribution. Uh, it's generally evaluated based on its utility.  Uh, and it also requires empirical validation. Right. But this the thing when you propose something is you don't just propose it.  You have to validate it. Often it require empirical validation so that the readers, people who read up on your research,  all researchers are convinced that your method is useful or reliable.  Okay. So you're proposing a new method to do something to achieve something.  Okay. So there's a methodological contribution. The fourth point is theoretical contribution.  This is more conceptual or theoretical or conceptual. So it's a concept or a model as a general, sort of like a vehicle for thought.  Uh, for example, a framework, a design space or a conceptual model.  So this one is slightly more, you know, it's I suppose this way it's called theoretical contribution is slightly more theoretical.  Uh, it's meant to reveal the nature of what is and what will be, uh, simple with something that's descriptive and predictive.  And it's meant to advance our understanding of a phenomena.  So this also falls into HCI, if especially if the theory obviously,  that you are proposing is for computing and effects of human behaviour in computing, then it becomes a theoretical contribution.  And again, it's not just a theory that you propose out of nowhere, because you can wake up in the morning and you thought about it.  Obviously you have to validate it.  You go through your literature review stages, you go through some validation processes to validate your theory and share it with the rest.  This one is getting a lot of popularity, mainly thanks to, I suppose,  this data set contribution where you propose a corpus for the community, for example, a repository benchmark task and actual data.  And you know, with AI booming AI is backbone is data.  So now data is everywhere and there's a lot of data contribution.  Recently, um, is common in AI algorithm operating system works and should include expression of its creation and gathering and,  you know, data set contribution. Well, a lot of research labs do that.  Uh, but also a lot of industries do that because the other one with all the data, isn't it?  Google has all the data. Apple. Um, meta. So actually, you know, big companies Google, Apple, meta.  They do actually also attend Chi conference. They also present their findings to present all their data,  the  thousands of data they collected, what it means, how it can be used by the community.  So it's something that, uh, is booming quite a lot, I suppose at in time is maybe one of the main pillars of, uh, HCI contributions as well.  Um, so what you do here is mainly your main focus is on data.  It can be anything, right? It could be movement data. It could be gesture sending data, it could be heart rate data.  And and then it is your then your duty to propose to validate this data, to show the process of how it's collected,  how it's collected, how data is clean, how it can be used for predictions or whatsoever.  And also its implication. You have to imply on what the data set can be used for so that people you know will actually want to use it.  And then we have two more contributions. The second last one is a survey contribution okay.  So this is a review or a synthesis of work in a specific area to identify trends.  And this leans very heavily on a literature review. Uh it generally only occurs after the area has existed for a few years.  I know I say area, I don't mean like HCI, you know, it's something a bit more specific.  For example, let's say you want to do haptics, right?  Haptics and haptic interfaces, touch sensitive touch feedback.  So then you need to find, uh, within the last few years, what, uh,  the key emerging research so you can group together and create a survey or a review or review paper.  This makes them in general very long to read, usually published in journals.  You know, I say long, I would say like ,  pages because they go through hundreds and hundreds of past research.  Uh, and it's also a very great resource pool because often even in your use case, depending on your research projects later on,  if you can find a survey paper or review paper that reveals the specific topic that you are interested at,  there's immediately a pool for   papers just from that one paper from that one research paper.  So basically it compiles everything. It tries to summarise everything.  And generally the, the title of for these papers would say like a survey of or a review of.  And then sometimes you'll say for the last ten years, for the last five years, you know,  a review of haptic devices in part for the past five years, something like this.  And it gets renewed too. So maybe after five years, another researcher will propose the same thing.  They will publish a survey the next five years that as, uh, as a researcher, as or as an author of a lot of these research papers,  I will say they're a little bit boring to write because you don't actually make new things.  You review papers, but boring doesn't mean it's not useful.  In fact, it's the opposite is probably the most useful, um, document for everyone else.  So it's something that is not the most riveting thing to do, but it is extremely useful for everyone to benefit from it.  Um, and I also highly, you know, recommend participants to work on it.  Yeah. So survey contributions and lastly is opinion contribution.  It is meant to persuade readers to change their minds using other aforementioned contributions.  So referring to other contributions you now try to form an opinion.  Uh, and the goal here is to be persuasive. And of course, again, it's not just a random opinion you had in your mind.  You took a bath. You got an idea? No, it's not like that. You have to be persuasive and you have to draw upon empirical results.  And you try to bring forward an idea that perhaps in some way, even fundamentally changes some of the procedures that we normally do.  Like I recall there was a paper a few years back.  So like, um, a lot of HCI, we do we do gather physiological data, meaning like heart rate, uh, skin conductance, uh, brain signals, so on, so forth.  We do a lot of these and we try to analyse these patterns as well.  And um, a few years ago there was a paper published saying that the methods, the how we do are wrong and here's why.  And then they, you know, they go through a lot of reviews and then they discuss about the procedures and  try to establish the limitations and how it can be improved in the future.  And they give they give their opinion on how it can be.  You know what's the better step of data collection for some of these?  And then there's obviously in depth discussions for them. So these are all the main, uh, contributions for HCI.  You can pick what you feel is most interesting to you and see if you can apply for your research projects,  but most often and not it will fall under the first two, which is either empirical or artefact contribution.  Okay. Somebody running upstairs.  Strange. Uh, so, uh, the this this seven, uh, contributions, they are listed in the, uh, reading material.  But if you want to learn more, especially if you want to go through a list of examples, this paper has it.  There's a clickable link. So it's not like a research paper.  Research paper, but they may look like a research paper.  Uh, it goes to all seven points and they list down examples, three examples of empirical evidence, a bunch of papers.  Initially I was thinking to list down or go through some papers for each of it, but I think it's gonna take a lot of time.  Uh, so I just going to link this and you can have a look at some of the past papers that falls into each category.  If you want to understand a little bit more about what each of them cover.  Okay. All right.  Um, a few more things to cover. Um, now, what time is this?  Ten, uh, spot ten. . You guys need a break on anything or any questions?  So maybe you can take a quick break. Uh, we can resume in ten.  No, it's . You can resume at . So.  Mr. Ferguson. Oh, yeah.  Okay. Okay. Yeah. Thank you. It's a a great one.  Just say it's better than. Most stuff.  Yeah, and then the old school. Yeah yeah yeah.  Yeah, yeah. Let's go, let's go, let's.  Go! I really want to see you here.  I was. On my.  Way to Los. I.  Understand? Why do you like to explain to me?  That you know what you're. Looking.  For. Yeah. Yeah, yeah.  I feel like. I didn't do much right.  I was like, somewhere in the. Sewer, so.  I'm just going. Two.  Oh my .  Is it like oh yeah, oh yeah oh I see I feel.  Like this is more.  You want to go through. Tear will tear from that side.  So. I put that together.  Yes. I love it. Oh yeah.  That's what I was telling you about this.  Yeah, I have one, I don't care, I just like, oh, I love.  Like, oh, yeah, I was worried about that. Sorry.  Yeah, just the other, like, you guys might have something that's not the technology is actually there.  So it's. Not like.  That is it. That. I would like to think that.  He was a soldier. Yeah. Actually, I haven't had one for work before.  Oh. I'm out. No I haven't. So.  And I saw. You know how I thought I was going to die?  I know that I can say I used.  To love that.  It's a great story. Yeah, it's a kinaesthetic.  I wasn't worried. About how I was going.  I. Really appreciate.  All the. Yeah, I mean, I wasn't going to do.  Really. So.  I'm going to give, you know.  You. The right.  Yeah. That's right. Could you do this on the.  You were in your there. Oh, yeah.  Wow. Yeah.  Let's go to bed. Oh my God. I think it looks like some kind of coke is coming.  I don't know.  I'm just thinking. Yeah, I lost, I.  Thought it. Was an hour ago.  Yeah. I think we were like. All right. You guys good?  Yeah. So let's resume, shall we? Well, before I, uh, plunge into the next slide, I just suddenly thought of.  I thought of my own sort of personal experience of getting into HCI.  So, uh, if you recall my, uh, academic history, my degree in masters was actually engineering, and then I decided to go to HCI afterwards.  And coming from engineering, you are very much more focussed on how realistic it should be.  Right. You don't really explore the crazy stuff as much.  You explore about the calculations, the design of bridges, the, you know, cat CAD modelling, manufacturing processes, things like this.  But I want to do HCI, and I recall that when I just stepped into Japan at the start of my PhD, my advisor told me.  What what is the, uh, the master's students?  We're doing a presentation of their projects at that time.  So why don't you go have a look and see what they do and get understand what's going on in the school?  Okay. Sounds fun. Uh, so, um, I didn't know what to expect because it was a media design school, and I'm engineering background,  so I went to have a look and the first like the first presentation from a student, it was called Kawaii Haptics.  You guys know kawaii is right? It's cute. Cute. In Japanese, it's called kawaii haptics.  I was like, what? What is this called? Haptics? And the students that are presenting.  It was a box. There was a box with a stick coming out of it, and in front of the box there's a display, the small display,  and on the display it was a little hamster, and in the display is shown the hamster biting on a stick.  And the stick was moving. So there's no actual hamster in there, but the display implies that a hamster in there.  And the idea is that you hold the stick, you get the kawaii sensation.  It's called kawaii haptics. Uh, when I first saw it, I was like, what did I get myself into?  I wasn't expecting this, you know, where, where. Whereas the things that people actually need or want.  And it took me a while to slowly digest and understand that this is kind of what you see I is which the first time you look at it, you go like, what?  But then afterwards, okay, there are some implications about it. Sure. It doesn't have to be a hamster.  It doesn't have to be this small box.  But this idea that a small movement of an object or the tactile sensation of an object can bring forward a certain kind of emotion.  Right. So, so I've been thinking about this a little bit deeper and about the implications that exists from a prototype like this.  So in this prototype it's called kawaii haptics. This is about a hamster biting on a stick that moves.  If you just look at it. In that sense, it sounds ridiculous, but try to think about the broader picture.  And I think that's sort of like an important thing to learn a little bit about. Okay, so let's continue the lecture HCI over time.  Uh, so, you know, computers are becoming more accessible, right?  Everyone has computers in their home. But at that point we also don't look at just computers anymore.  Everything else becomes more and more accessible with computers being everywhere.  Every other tools also becomes everywhere. Uh, the left picture is actually one of the very first, uh, brain computer interface.  And it's, you know, hooked up to a bunch of machines with a big sort of hardware and,  and so on and so forth, uh, able to read brain signals through the sort of skull.  And today we have very small wearable devices, some of them even dial integrated into other devices, like into VR headsets and so forth.  So you buy one of these, a consumer ready, you can actually buy them and start looking at your brain signals.  Yeah. So they become extremely accessible. Uh, they are not the cheapest thing in the world.  But if you can buy a $, phone, you can buy one of these things, basically.  Um, but, uh, one thing to keep in mind, though, is that HCI tends to lack a bit of a longitudinal study.  So what? As long as your study is basically a long term observation and data collection process?  Uh, if you look at other fields, you know, more sciency fields, it does tend to, uh,  have more of such studies where it's deployed over years, sometimes one,  or  years.  But you do a long data collection and then you draw some conclusion from your analysis of the results.  That tends to not happen in HCI as much, and I do hope there are some you know, I'm not saying they're not at all.  There are some. But usually it's also like a few months not to until the point of a few years is a little bit rare.  The reason is because HCI moves too fast, right? The next day we have a new phone that does this and that.  Next day we have a new VR headset. The next day we have a new AI model that can predict the new sort of next line of crazy stuff.  It moves too fast. There's also the reason why HCI actually, when we publish research in HCI, we target more conferences now in the publication space.  There's generally in general, you can see there's conferences and there are journals.  And if you have friends in, you know, postgraduate studies in other fields, at least they tend to target in engineering journals,  a lot more research journals and then over time for journals is even a few years.  You summit one time, you get rejected and you just meet again. And then as you revise, revise.  And then at that time, two years have passed, at least because of how fast HCI is and how fast new tools keep coming up.  We also target conferences, which happens like Kaist once a year.  And it's not the only HCI conference. There are a ton more conferences that you can target every year.  So I just want to say is that it moves very fast. Yeah, we get new tools every day.  And this is something that if you want to propose a research topic, I always do.  This is more for like master's or PhD.  But if you propose a research topic, I always advise to think about society at least minimum five years into the future.  Because if you don't do that within the next few years, Apple will create a product that does what you are envisioning.  Google will create a product that is what you envision,  and then you're screwed because then your work is not novel and novelty is the main point of your research, right?  Uh, it can also be quite complex. Not saying that other fields are not complex, but HCI is complex in its own ways.  It prioritises humans this way, human communication.  And in a lot of cases, uh, because of this, finding your target users is always challenging.  Uh, but necessary. Challenging but necessary. Uh, and it can require you to collaborate with a lot of other fields, experts from other fields.  If you want to do your research on dementia patients, you don't get dementia patients.  Research on Maori, get Maori and research on lawyers or lawyers get lawyers.  So whatever it is, if you target something, you should actually co-design and collaborate with them.  Um, and that's there is sort of, uh, the general challenge in HCI because of.  Fine, because it is obviously not easy if you're just a student in a school, you know, uh, to, to reach out to some of these other groups,  to reach out a doctor in a hospital, say, hey, can you stop your surgery for a while and just talk to me?  And it doesn't work that way. Right? So you have you have to, uh, this kind of arrangement is going to reach out.  Takes a while. And obviously, you cannot just talk to one person to it.  You have to have a few to get the feedback from. So this is one of the general challenges, uh, in HCI.  Um, okay. Understanding HCI research methods.  So this is just a very high level touch on the research methods of HCI.  Um, this is also something that applies to most research that it should be rigorous and relevant others,  and it is never sufficient to just develop a new interface, because it's very easy to anyone who just say, like, I don't know, make a new interface.  Uh, but you should always consider what is the need for it and what are its effects.  You don't just say that you develop a new interface.  And in someone, if someone asks you, why are you developing it, you should be able to answer instead of saying, because I thought about it.  You you have to say you're developing it because you've established there's a gap in this kind of interface that does not exist,  or it can be improved, or there's some kind of clear problem statement that you can share.  And then afterwards also understand how it can potentially affect the user in some way.  Is it more enjoyable? Is it more usable? Is it more interesting?  Whatever it is. Okay. It should be some way to evaluate it. So.  So and it's on the human side. So that's again the main point HCI is on the human side.  Understanding users feedback is very very important. Uh, you study designs can be challenging.  So, like every studies, they all have their own challenges.  But what makes HCI, uh, challenging is because specifically the human side.  Because you ask, you will come to ask yourself the question throughout the process of, you know, being a researcher here.  How do you evaluate the performance? If so, how do you evaluate it?  Do you perform quantitative or quality analysis?  Do we compare with a baseline condition or do we create multiple versions of something and then we compare with each other?  And it's also quite often to design an entire task around the system just to evaluate it,  even though it was primarily not what it was originally designed for.  I can give you an example. So this is this is one of the research.  I work with some students actually doing my research a few years back, uh, maybe like five years back.  Um, the idea is to, uh, what is it, develop a handsfree interaction in the virtual space in VR, hence free VR interaction.  And I propose two interaction methods within the virtual space.  One is the use of eye tracking. Second is the use of muscle activation.  Like like muscle activity, like flexing. You know, muscle activation.  So muscle activation is called biography, electromyography.  In fact, this little armband here is it's called the meal armband.  Myo uh mu armband. It senses the muscle activation of the arm and it's eye tracking.  And keep in mind, at that time, there was no really consumer eye tracking device within a VR headset.  So actually, that that that is like the very earlier version of Oculus headset now owned by meta and had to completely destroy it,  to be honest, had to cut the lens, put the camera in there and then make sure the camera software blah, blah, blah.  So in the end, the eye tracking works in the virtual environment, and you also get a signal for muscle activation.  And so the idea is that you use the gaze for selection in a virtual environment, and you use the muscle or muscle as activation.  That way you have minimum movement. And it's sort of like, uh, gesture free activation and navigation, eye gaze and muscle.  So actually, on the screen days, uh, Fitts law is actually a version of a Fitts law, which you will learn in week four.  You will learn physiology. But the the point I want to see is you need the second screen, which is the evaluation of the performance.  And I was thinking what kind of, you know, potentially real world or like VR application performance that we can use to evaluate a system.  So I made a basically a shooting game, VR shooting game where you get to look at the targets,  the targets randomly appear, and as you look at the target, you sort of activate your muscle to shoot the gun.  And then I can evaluate how fast you do it, how accurate you do it.  And this lets me understand the performance. So this whole sort of setup, it was not primarily for shooting game but it can be used for evaluation.  So you'll find yourself often needing to design entire tasks just so you can evaluate your system.  And you know, this also leads to very difficult metrics that you need to measure.  If it's just like time and accuracy, sure, you can log that.  You can calculate how close the shooting point is to target or how, you know, log, how long it takes for them to complete the task.  That part can be done. But the hard things to measure is, for example, how do you measure player emotion?  How do you measure their enjoyment? How do you measure a sense of togetherness?  How you know, these things are quite challenging and most often a lab setting is actually not sufficient.  So sometimes you need to get off the lab space. This is another thing that's quite different.  Do you do research in other fields? Usually it's purely lab space.  In HCI, it's not necessarily only lab space.  You will find yourself walking around, maybe public demonstrations, uh, workspaces, um, trying to get feedback in real world scenarios,  deploying an app over the period of a week and let people use that and gather data, things like this.  And and then you need to, you know, think about how these things can be measured.  Well, quick question. How do you think that can be measured? Anybody? So let's say you want to create a system.  Let's say you create a new zoom interface.  Right. And you want to measure the sense of togetherness that you say is improved over a zoom interface.  So how do you measure a sense of togetherness? Yeah.  It's nuts. Sorry. Thanks. So I can get it.  And a survey. Yeah. So a survey is something that you can.  Definitely. It's something that we all do. So like a questionnaire or a survey.  Right. And well, I mean, there's something that you can. We'll be covering the next couple of lectures.  I don't get too deep into it. Uh, but the idea is that you can use surveys, you can get feedback from there, and then you can answer,  like from a scale of  to  with your sense of togetherness, something like that.  Yeah. But that's just one of the ways. And there's a lot of other ways you can also evaluate.  You can try to think a little bit more or something more objective as well.  For example, like emotion does. That's, uh, increase heart rate, shows a higher level of emotion.  And you don't just make the claim.  You do some pilot studies or you do literature review, understand what's the state, what people have done in terms of heart rate with emotion.  Is this a valid method. And you find it to be valid? Okay.  And you start measuring  because you also have to understand there's other factors that influence heart rate besides emotion.  Right. Then then a lot of context will come into play. And it's your duty as a researcher to think a little bit about it.  Um, as you go forward. Mhm.  All right. Interdisciplinary research in HCI.  Um, and I think, I think it's pretty well established now that HCI is quite a mix of a lot of things.  Right. Um, some fun, some not so fun. Depends on your preference.  But I would say that in a very general case we look at these sort of three main pillars.  Uh, computer science, human factors engineering and cognitive science.  There's a lot more. So actually, you know, it's like design psychology, social science, biology, physics.  But again, all these also tend to fall into this sort of three circles.  Computer science, human factor engineering and cognitive science. And very occasionally you do break out of this circle as well.  Um, because it's not like we we don't try to define the kind of, uh,  expertise or feel that you want to mix into, uh, your evaluation of human factors in computing.  Uh, it can be as wide as you need, um, you know, uh, to, to, to suit the research scope.  So, of course, this also leads to some of the previously mentioned challenges in like,  how do you evaluate, how do you find the right person and how do you recruit the participant.  So on and so forth. Um, but at least to me, I find it more interesting.  I find I feel I feel like the best research out there is the kind that mixes various fields,  research fields into a pot, mix them up as opposed to just the, you know,  I mean, I'm not saying they're less or more important, but at least to me,  I find it to be more, you know, interesting, um, for this kind of research to work.  And also, it allows people from different disciplines to be an HCI researcher.  I recall there was a PhD student last time that I worked with.  His background was accounting. But after a year he built an eye tracking device.  So you could you can still do that. It's there's no limitations, uh, on what you can achieve in HCI.  Um, yeah. So I will give you another example of a research work.  So this is also another one of mine, um, a few years back, uh,  on how you can try to combine the different fields and envision a scenario that's very out there, very out there.  And I use this as an example because I like it in terms of how out there it is debateable compared to like how AI haptics, what you'll see.  Well, I guess it sounds no necessary. In sociology.  A conversation between people depends on their distance, eye contact, orientation and arrangement, which is called a formation.  In this work, we introduce mobile conversational paradigms that redefine that formation by introducing the concept of multi-directional conversations.  The displays are mounted in front as well as to the left rear and to the right rear of the HMD.  These displays act as a multi-directional video stream that can be viewed from all directions.  The rig also consists of a helmet fitted with a mountain top for a  degree camera for viewing methods.  We developed two variations. Equal rectangular view mode allows the user to see the world with  degree body in a rectangular format.  In levy mode, users can see two overlapping transparent video fields with  degrees of each.  As for the display method, we developed two variations view sharing and gaze sharing.  In Qaddafi mode, the observer can only see the section of the internal display that the user is looking at.  In shared gaze mode, an observer can see a pair of eyes that will look in the direction that the user.  So let me give you some context. Maybe it's not the easiest thing to understand.  Or is it easy to understand? You know what this is about? No.  Yeah. So in the past few years, there has been with the emergence of  cameras.  Right. It allows you to capture the full . Right. Maybe some of you guys have seen  cameras.  Um, there has been a lot of research on how to give such  vision to humans, how we can augment human vision to be able to perceive  space with.  Clearly there's not something that we are designed to do by nature. But what if we could?  That's the idea, right? And then what does it imply? Does it means better spatial sense?  Is it better safety security. So on so forth. Could be that's the implication.  So there was some research about this. And the past research have shown that, you know,  if you attach the  camera on the head and you equipped a viewport where you can see the full view, it's called the rectangular view.  Like you saw that, it allows you to see full  within your sort of narrow vision.  Then it does increase spatial awareness. So that part has been explored.  But this work actually pushed one step further. So the idea here now is that let's say everyone already has  vision.  Let's say we live in a future that everybody has augmented vision. Then what does face to face communication actually mean?  Because we do not need to be face to face, because we can see in all directions every time, anywhere.  So I thought that was interesting research question. And it's something related to something called formation facing formation.  You can look it up probably like social science. So then this becomes a merger between HCI and social science and also obviously computing behaviour.  So the prototype here actually shows that for the user they can see  vision.  The challenge becomes how to communicate where they are looking at to the outside world.  So this means that you need displays three sixes. So when the person is looking behind the display shows that you are looking behind.  So that's what it means. It becomes uh uh, formation analysis and how you can redefine how people interact with each other in social space,  and also about how to come up with new, how to propose new formations, new facing formations in a world where everybody has augmented divisions.  Right. So this is something that you sort of project into the future ten,  years into the future,  and imagine how interactions like this would be like and what kind of implications it brings.  Is it crazy? Yes. Is it fun? Yes, absolutely. Okay.  I think it is. Okay. So we discussed a little bit about target audience earlier, right about um,  how we need to recruit the target users depending on what we are proposing or, you know, what research projects that you're working on.  You know, think a little bit about your target users, not not all the time that you need to reach out to a doctor, but it depends on the topic.  Um, and that's a very important consideration that I like to discuss a little bit more.  So and most hit share research.  You look at the passwords as well, but they tend to target other researchers.  Um, and I would say is because a lot of researchers actually do not get trained based on the societal impact.  So they will propose an interface, and then they'll recruit like  students or  researchers to give feedback on the system.  Yeah. And this exists everywhere. And I think there's room for improvement.  I'm not saying that. I'm not saying that this isn't wrong. It's not at all because I also do it.  You know, I also have research systems like this, the ones that I show you.  I recruit students to evaluate and give feedback. So they are also researchers.  But I think it's important to think a little bit more on the societal impact base.  Think about how you can influence society and do not do research just to get good citations or just just to get your paper out there,  or just to get a doctor in from your name. That's not why you do research. Um, because your target audience is the real world.  So I'm going to share a little bit more of, of, uh,  some of the research in the past that is more focussed on a specific target user that is potentially difficult to reach,  but it's still important to consider. So this one is called, uh, dementia.  Dementia ice. I give the example of dementia earlier because I wasn't dementia with dementia patients before.  I'm not sure if there's something we just know. So on. Um, yeah.  There's no cell. And maybe I'll just jump to the interesting part.  So the idea here is actually, uh, a system, a device that can simulate the vision of a dementia patient,  how dementia patients visually perceive the world, because dementia is a very complicated neurologic, neurological sort of, um, issue,  and because it comes from the brain, it influences a lot of things, including how we visually see the world.  So to get such information,  we have to directly communicate with the patients and the practitioners to understand how they we cannot just decided, okay, this will blur you.  I'm gonna make it blurry. You know, we have to decide this based on what the experts say, either the patients or the doctors.  So this research project, entire project is a direct collaboration with the medical consultancy group,  which links us to the hospital and also the patients, so on and so forth.  We ran out workshops, and we came up with a range of sort of visual symptoms just to simulate.  It could be blurriness, it could be narrowed vision. Uh, but there are a few more.  I don't recall the names anymore. I think there's like  or  overall symptoms that you can toggle and view the world you the will.  So this is actually the process. Like this tissue box actually looks the same to a dementia patient which is kind of interesting.  And then we got some interesting feedback too. Like there was a hospital. The roof, the ceiling of the hospital had like lines on it, squiggly lines,  which is not a big deal for non dementia patients, but for dementia patients they look like scary rooms on the ceiling.  So then the idea become how do we simulate this to create the sensation.  And there are a lot of implications. It could be empathy.  It allows you to understand the patients a little bit more and change your way of interaction towards them.  But more importantly, it can also be used as an assistive device.  By using this, we can now think, okay, I shouldn't make the ceiling look like that.  Then you change the sleek design and you know, if the if the lighting is very poor and okay,  and I add some lights here and so that they can sort of adapt to this space a little bit more.  So it becomes an assistive tool as well.  But uh, at least the main message that I want to take across here, bring across here is the collaboration with the target user.  It's not easy.  It's not supposed to be, um, but by doing so, it make sure that you have a very strong societal impact in what you are trying to do in your research.  And again, it's not like all research has to be like this, but it's just something to consider.  Yeah. Okay.  So just not with like just general societal impact. But how about industrial impact?  I think some of you are also interested in that. Right. So um, this this would usually mean, uh, the evaluation of a specific product,  depending on whether you are a researcher in a company or collaborating with a company, then it's a bit more of like an industrial impact.  So, yeah, researchers occasionally collaborate with companies to achieve this, myself included.  So I also share, uh, research like this. Um, so basically, if you are an academic researcher, you may reach out,  communicate with some of the big companies, tech companies, and you say that you want to evaluate their system.  It becomes a win win situation. You get to publish the research paper of the novel prototype you are making,  and it also helps them publicise the, uh, you know, systems a little bit more.  Uh, so last, last year, no, two years,  or  years ago, uh, we worked on a project with Google,  actually is Google Atap, which unfortunately already dissolved. So you can find their page anymore is Atap.  They worked on something called the soli sensor. You guys anybody know what is the Google soli?  Yeah. The thingy. Yeah. So the, uh, on their pixel four is pixel six, one pixel four phone.  They deployed something called the soli sensor on the phone, which allows you to do micro gestures over the phone with non-contact,  and you can interact with the display so you can do left right, then tap, and then you know a few things over the phone and you can interact with it.  And now it's I think it's still being used in like the Google we call it the home, the nest devices.  You can use a nest because the voice is gone already. Unfortunately, you don't use anymore.  Google has also gone. A few years ago they had this soli sensor.  It's called is a radar. Actually, it's a radar interface.  And this is a work that I work with. Uh, also Daniel and Tamal, which is one of the one of the, um, tutors for this course.  So I'll show you the video. For radar, for proprioceptive gestures.  The proprioceptive sense is a human ability to sense distinctive topographic features of the body without a vision.  Gestural interaction with proprioception in mind would be a powerful solution to operate wearable computers in a compact interaction space.  Previous projects tackled on skin user interface designed with various sensing methods such as acoustic sensing, inertial motion sensing,  computer vision, and electric wave guiding radar, Hand utilises Google's Soli radar sensor to observe the gestural input.  While we clear challenges arose with the previous projects, radar sensing won't get influenced by climate conditions or acoustic noise.  Unlike camera based tracking, we hardly see privacy concerns since it cannot capture photorealistic images.  A solid state radar won't cause mechanical problems, doesn't consume much power, and is immune to motion noise.  Soli sensor got a wide field of view of  degrees, which would create enough space to make a hand and its surrounding area an interaction space.  We have conducted three studies in this project.  The first study was to determine the level of proprioception for topographic regions on the back of the hand, for better gesture design.  We introduced the end bank task to induce participants cognitive load while performing gestures,  and Fitts law to calculate the index of difficulty to reach each landmark.  The results show that the distance and target size affect gesture performance.  Movement time increases as the target moves from thumb to pinky finger and knuckle to fingertip.  We also investigated social acceptance and preference for proprioceptive gestures with the participants.  The survey feedback results matched the result we saw in Fitts love based study.  In the second study, we designed a DNN model based on the deep radar net for gesture inference and collected gesture data for the model training.  Deep radar net takes a complex range. Doppler map from soli as an input frame model will extract the salient features in each frame.  Then the process data goes through temporal model with previous buffered frames to infer perform gestures.  Based on study one results.  We designed radar hand gestures with some false positive gestures to make the model robust and immune to non-related body motion.  Study participants performed each gesture in two seconds, ten times for four sets.  We designed various sets of different gestures and tested them in low.  So maybe just in the third study, skip forward on sensing.  So this study is real time in relation to backpack and a soli sensor bangle for the left wrist.  Participants could utilise a console app to receptive interface either focus the body part ownership.  Yeah. And then the last part is actually the potential application that can be chosen deep in the system.  Radar hand could provide more proprioceptive alternatives to computer aided design workflows,  to work with creative software on a computer or coupling radar hand gesture input with hand tracking in VR space,  we can supplement more interaction to the VR experience. Some parts of our daily life need more focus, including driving or cycling on the road.  By taking advantage of the proprioceptive sense, the radar hand system would be able to assist AIS free interaction for road navigation.  Additionally, we envision that Radar Hand is also usable for users of prosthetic hands.  By leveraging proprioception, the user may gain a better sense of body part ownership for more details.  Okay, but maybe I should explain what proprioception is if you have never heard of it.  Proprioception is the sense of body ownership, sense of body and limb placement.  If that's hard to understand, think about this. Close your eyes and touch the tip of your nose.  You can still touch the tip of your nose because you know exactly where your nose is.  Even if you can't see it and you can see your finger. So you have a sense of proprioception.  And the idea is that on our hands we have also a sense of proprioception,  especially things that stick out the knuckles, stick out the fingers, the fingernails stick out.  So how do we use these landmarks as gesture? As a gesture sort of platform?  Uh, using the radar sensor. So anyway, uh, the main point I'm going to say here is the industry collaboration with Google.  And that happens quite a lot, uh, where we work with, uh, target companies.  And last one or not last for this slide at least is uh, policymakers.  So you is quite often for researchers to also collaborate with policymakers or even like the  government to create guidelines or frameworks for updating policies such as new accessible features,  ergonomics of a product.  I myself haven't really worked with policymakers, but I do know a few other researchers who are working with policymakers to create sort of new,  new AI guidelines that say, oh, new XR guidelines. How do we use these these tools, these new emerging tools for future interfaces and designs?  Okay. The next part I want to talk about, uh, well, actually is the main sort of last section ten to revise.  Okay. The main sort of last section we talk about is the trade offs in HCI.  So I think it's important to also acknowledge what are the things that we potentially or the limitations or sort of what we sacrifice,  uh, in pursuit of, you know, top HCI research outputs.  Uh. The thing is that we always try to go for a better solution instead of an optimal solution.  And this can mean, uh, a few things. And also because I guess we can't please all stakeholders just because, you know,  you get feedback from five doctors, they don't all always share the same opinion.  So you have to find a way to balance between this, the feedback that you get to design your your interface.  And actually in general, a very good example would be like a keyboard system, right?  The left layout keyboard layout is something that we are all familiar with is the standard quality keyboard.  But maybe you already know this, maybe you don't. It's not actually the best layout.  So what is the best layout? It could be something that makes us most efficient, like the fastest in typing or the most comfortable.  Both of which are not for our standard keyboard layout, but we still use it.  Like if you look at the other layouts, the top one, the middle top one, that's more like an ergonomic layout,  which is much more comfortable for your hands, you can use for a longer period of time.  Uh, the third one is that the track layout,  which looks strange to most people but has proven to be the fastest typing layout and the top one doesn't look too different.  But if you look at it properly, it's basically it's not arranged in a way like like this,  like normal keyboards is an also linear keyboard, and it's still quirky, it's still quality.  But if you try this out or you've never tried it, you suddenly try this out. I promise you, you can't type, uh, very fast at all.  But if you were to train to use any of these other keyboards, like the also linear for example,  you will find yourself being faster and more comfortable to use the keyboard if you were able to reprogramme your brain that way.  And yet we are still using the standard keyboards in all our keyboards, right?  The quirky layout that we all know and love. So that's a bit of a problem, isn't it?  Um, and simply because we are used to it, we are just used to it.  Yeah, we know that we can type fast, and we know that our friends can type relatively fast with these keyboards.  So if you pass to a friend, they don't need to relearn like you did.  Uh, if you want to try any of these keyboards.  So there are other more practical reasons to why you want to stick to the more standard layout, even though it's not the best.  And and some of these other outputs, you know, they are actually output from HCI research, right?  As the design is layout, the design, how the buttons should be placed.  And they they do experiments, they let people type on it and train over a period of time.  And they define them to be better in a lot of cases. That's the thing too, isn't it?  Like humans, we are cognitively very malleable.  We can train ourself to do things that we didn't expect to be trainable, but we can do that.  It just takes time and effort, isn't it? Which is the thing that people don't like training.  Uh, time and effort. Um, okay.  Um. Uh, the next point I want to stress on is the intersection between usability and security.  In HCI, we want something that is % easy to use, but if you are a security guy, you want something that's % secure.  If you want something that is % secure, it will not be % easy to use.  And that is like one of the trade offs to consider.  And it's very important to think about. Right?  Because the more secure it is, the harder it can be to use, especially if you use like, you know, banking apps and so on and so forth.  Right? You put your touch ID there or your face ID or whatever it is, and then it give you uh, maybe, uh, a word that you,  that you recognise and you still type in a password and then they send you an email and then you add an authenticator.  The more you do this, the safer it is. But the less usable it is, the less easy it is to use your platform.  So what do you prioritise? Um, that's a trick question.  I can't say there's a full there's no definitive answer to this.  And like everything else, it depends. You have to have some kind of trade off.  You security is obviously still important. So perhaps you as a, you know, HCI researcher, you try to find out first what exactly is % easy to use,  what kind of interface actually is one of the things you and then as you inject more and more security features, how much do you actually sacrifice?  And at which point a user feel like it's a good trade off.  And just because also an example like this,  especially just because a user feels that that point is a good trade off, doesn't mean it's still the best thing,  because you as a or like a the security guy that you work with,  they know that if they were to choose this method that the user has selected, then you get hacked the next day, right?  It could be. So there's a lot of things that you have to consider. And it's a huge it depends because so there's no definitive answer for that.  But you try to establish what is the trade off. Like. You know, if you look at some of the examples I give you to, isn't it.  There are trade offs in some of the prototypes that I show you. The radar hand, it looks cool.  You can do all the gesture thing, but it's clearly a big wearable device on the arm.  It drains well, it doesn't drain, but it does take a bit more power.  So you have to strap a battery there and so on. So the trade off becomes the general sort of variability of everyday activity.  It becomes a bit more difficult to use. Right. So there's a trade off.  And we have to discuss these trade offs, these limitations when you pursue your HCI research.  Okay. Uh, what else do I have? So I have shared most of the main content I want to share based on also the reading material.  I hope the general comprehension on what this is, uh, helps for you guys.  I do want to share you guys like videos. I have one more video to share.  So I show you one more cool video. This is for another HCI conference.  So Chi is the top conference. But like I said before, it's not the only, uh, HCI conference.  There are a few more. One model is quite well known. It's called newest user interface software and technology we call it, which is also, uh,  sort of like a yearly conference and with unlike Kai, prioritises more on the fancy stuff, the cool artefact contributions.  So I thought I'd include that just to, you know, get some blood pumping to feel the excitement of HCI.  This is actually by better because this is better as research. If.  Okay, well, I like how you always show us your videos where they separate different, you know,  categories so you can understand the idea of design, haptics, AI interfaces lets you understand a little bit more of the contributions in HCI.  Okay, so that's that. Um, let's have a casual activity.  Yeah, a simple activity. Okay. So this is just sort of like a five minutes discussion with the person next to you.  I call it think per share. Okay. So in five minutes just discuss with the person next to you.  Uh, if you get the design, a new interface, interaction or device together that can generally enhance teaching and learning.  Yeah, I'm just scoping it down a little bit because I say if anything it becomes a bit too much.  So let's say for the context of teaching and learning in school or otherwise, what would it be?  So just discuss with your partner, the person sitting next to you,  try to come up with something interesting, and then what you should consider to design and develop it.  And then I'll just go down a little bit, you know, get some ideas just to get some ideas flowing.  Yeah. There's no is just a discussion open discussion here. So five minutes.  Uh, now it's   I will bug you guys again.  And then just to catch some of you and share your ideas that you came up with.  All right. Okay. I think.  We have the microphone. Right. I.  Something like that because Microsoft.  Said. Oh, God.  But. You know.  We'll see. Us.  So that's how it was.  Like you. Just.  Like the. This is one very clear.  Clear. Yeah.  Yeah. That's not.  Oh my God. I was like, I'll just throw.  That out there. Oh.  Yeah. I got carried.  It's like, hey, yeah, yeah. Yeah yeah yeah.  Because it is responsible for creating life for us. You know.  Maybe it. Doesn't work.  Maybe. That's why you have.  It's more than just. Like vintage lipstick.  Are you leaving soon? Yeah, definitely.  I'm so sorry. Yeah. So after this is the project in production.  Let me say for about ten minutes today. Okay.  Maybe expanding on that as a reminder, but it's not a requirement to come back.  So it's time. For them to go for it.  Oh, this. Is so this is what I.  Yes. I was not sure we could.  Yeah. Yeah, yeah.  Because of that also we're going. Somebody else.  Said he tried to. From side to side.  So do. We feel like.  From the side of the fence. Yeah.  So what we've. Been doing about.  This is that if.  I think that there was something said the same age.  So. You know, you think of this.  But it's. All right, guys, let's do some very brief sharing session.  Yeah, I mean, it's casual, so don't feel stressed. There's no right or wrong. Yeah.  Just share some ideas. So basically any kind of interface design uh, for teaching and learning.  And how do you plan to approach it before I sort of go around, does anybody want to volunteer and share what they came up with?  Something groundbreaking maybe anybody? Yeah.  No. You guys want to share?  You can have something like. Something like we are of your classroom.  We've got classrooms so we don't have to come on campus.  Uh, so you feel that the virtual classroom can be as potentially as realistic as actually coming to the classroom?  Yeah, I think so. Maybe several years later. Mhm. Any idea on how you plan to approach this if you are the designer of such a system?  Um, just like the thing I'll have, I'll have that one on a hat and then I'll create a virtual wall and then everyone go through the room.  So, um. Yeah. Yeah, yeah. Thank you actually.  Did anybody else come up with similar idea, like a VR platform?  Nobody. Okay. Pretty level. I saw a video with, uh, Japanese.  Japanese lecturer is doing the lecture in the virtual.  Right? Yeah, actually, you said a few years, so it is pretty much already today.  Yeah. You can find these platforms online even. You don't even need a headset.  Actually, some of them, you can use the web VR if you don't have a headset. I guess one of the issues is that everyone needs a headset, right?  A VR headset, if you want to envision this future and then maybe obviously not everyone has a headset,  but there are ways that you can also use to circumvent that. Yeah.  Anything you want to add something else? Um, well, yeah.  Yeah. So such platforms already exists. Um, and you will find actually also studies that try to do this remote learning.  Uh, I remember there was one very interesting research paper too, about how difficult or how,  how can a teacher increase the understanding of the students when it's all virtual.  So everyone's like avatars, right? Everyone's looking all strange avatars and so on.  So how do you keep an eye on everyone and also have a deeper understanding of connection with the students?  And there was a really interesting research of having, uh,  single avatar or a single virtual agent in the virtual classroom that actually embodies everyone's cognitive state.  So for so the teacher can look at this avatar and understand the general flow of everyone, like, is everyone feeling stress?  Or is everyone feeling happy or content with the research just by focusing and interacting with this one avatar?  So I think that was quite a interesting research as well. And it came out right.  Any more ideas? Anyone's still on a volunteer. Do you have abundance?  Do you discuss together? Yeah. You guys? Yeah. You sure?  Oh, well, you can say they're a type of monitor to, uh, to know the student's emotion in the class.  Uh, if somebody is happy or, uh, tired or boring, uh, and, uh, it can, uh, uh, uh, it can evaluate the, uh, effect of teacher's class, I think.  Any idea how to sense emotions? Mhm.  Cameras. Yeah. So all eyes, uh, can use Apple Watch to, uh.  Yes. Uh, monitor the temperature or the, uh, heartbeat.  Yeah, yeah. Maybe it's a computer vision techniques because definitely viable to capture emotions.  You feel that any trade off of this idea. Uh, do you feel there's any issue potentially with this idea?  Every every research has a trade off limitations. So it doesn't mean that your your research is a problem, but can you think of one?  Uh. Uh, I'm not sure.  Well, uh, refuse to use it, so I suppose that could be one of it.  But, uh, one of the problems with this, a lot of emotion research is always the privacy at that point and uses a use camera.  So now we have data of everyone's faces, and we have data of what everyone's feeling like.  You probably feel quite stressed if you know that everything that you're feeling is being translated to the teacher to understand it.  So there's also other implications, but there's also research that's exploring what you are proposing.  So that's a very good proposal. That's just uh, maybe somewhere in the middle.  You guys in the middle want to share something. Can you pass it to the in the middle.  And with that. Yeah. Like, um, I want to get the application in the application pro, like.  Yeah, you can, uh, it's more like a application.  You can, uh, that can teach you, uh, how to make, uh, uh, um, I think.  It's more like, um. A to teach you how to make a name.  Makes, uh, uh, what you want to do, uh, step by step as that step.  So, um, I will use the camera to catch the more information.  Yeah. Any example of what you want to learn with vision, uh, so that you feel maybe it's more effective than standard learning?  Is that you? Are you thinking like a class like this or like learning a new skill?  I think it's a new skill. Um. Uh, I can give you some suggestions.  Yeah. Thank you.  Yeah. So, um, that's actually a good idea. And there was a couple maybe.  I'm not sure if some of you guys have seen some of the videos of what people have used Vision Pro for.  Um, one of the very interesting ones, because it's mixed reality.  Right. So you see the physical world with the virtual overlay.  Um, piano is something that was quite interesting because you can overlay a virtual piano over a physical piano,  and then you get to see the thing comes down, and then, you know, you press it the same time and you're using the actual physical piano.  So I think that's actually a very, very useful tool for learning because it bridges physicality.  You're not just watching a YouTube video, want to learn piano?  And I can't say it's better than an actual, uh, teacher teaching you piano.  But the other implications, like maybe it's cheaper. You don't have to get someone a tutor to teach you how to play piano.  Um, you need to conform to the schedules.  In this case, you can learn by yourself with the actual physical piano. So actually, that's one very good application of, uh, mixed, uh, solution Pro.  Maybe you can get one more example somewhere in this corner here. Uh, unless there's anybody who wants to go.  Perfect. Uh, I have some idea of the, uh, in language education error.  Um, you know, language allocation is, um, heavy communication.  So I have two ideas. Why is one is very expensive hardware, uh, hardware needs, uh, is lack, uh, smart, smart communication a hub B yeah,  yeah, we we can put some high quality device, a microphone and AI technology, virtual virtual teachers.  Like, uh, you can talk with uh, I the, the to the daily and playing games, but I think more, um.  What we need to know to enhanced, uh, language education is some basic, uh, applications, you know, you know, uh, education.  Yen. Language tutor. Uh, human tutor is still very important.  Uh, so. But they need to make some, like, games, like, you know, snake and ladder games.  Yeah, yeah, but they, the tutors, they maybe don't know how to use, uh, coding to make, uh,  snake ladder calls so we can make some platforms that, uh, provide, uh, this, uh, applications to make, uh, no more, uh, not no idea.  Uh, people can present that. That's their their news.  Yeah. Yeah. Thank you so much.  I really like language learning ideas because, uh, I myself, you know, I was forced to learn Japanese when I was in Japan.  Not fun. Not not very fond of it, to be honest.  And it's it's it's always very challenging if I feel learning languages, if you are bilingual, trilingual, then kudos to you.  It's it's very good, uh, that you're able to pick up multiple languages.  It's very challenging because it's it's a lot of memorisation, but also a certain part of, of logic, I mean, I couldn't quite put my finger on it.  And because of that, there's also a lot of really strong research on new language learning processes, you know,  things like and I tried to leverage learning methods like imitation learning or putting yourself in the shoes of another.  That's why I there's also a couple of VR research where you are in the perspective of a, uh,  citizen in Japan or like, you know how they say that the best way to learn is to teach.  So there are there are systems where I put you in a place as a teacher virtually, and then you use that as a learning process.  There's also, uh,  there's actually a VR game on steam where it puts you completely in a full virtual environment in the city of where you want to learn the language.  I think that's quite good. So you, you get to see you hear people walking around speaking the language.  You can see the subtitles and you can go to a restaurant, you can order food, you can see, look at all the I like to try that myself.  I haven't tried it, but I watched the videos on steam for the name.  But I guess you can look it up like steam VR, like language learning server or something.  So yeah, I really like the idea. Thank you. Well, we we are sort of running short on time, so I think we can stop the pressure activity here.  But you know, whatever ideas that you have, maybe just write it down. You never know what'll happen in the future.  Okay. So yeah. So that's basically it for the main lecture.  The third part, the final hour will be on, uh, research projects.  And this is the part that's, you know, quite important for you guys. All right.  So let's move on to the research projects.  I will do a sort of brief understanding on the layout, and then I'll pass it to the tutors to share the research projects.  So, um, these points are all basically mentioned.  There's a separate file on canvas on your. Research project, so please check it out.  But this is a sort of quick, digestible way to look at it.  And one is going to be a team based project because of the total number of students we have in the class, which should be .  It doesn't look like ten, but it should be . There will be about  groups in total with six members per group.  Okay. It's common when we come up with a project based course that there will be people asking, can I make it seven?  Can I make it eight? Ultimately, the instructors will decide and we can make it seven.  Unless other groups have less than six.  So if you request for seven but there's a group that's five. Then I just asked one of you guys to go or assign one of you guys to go do this.  Let me get six. Well, if everyone's only six and it's full and you need to be a seven, of course it can be a seven.  Okay. Well, prioritise. You've tried to make it six and even members per group they don't.  You need to select your group members on the Canvas People tab. So just go to canvas go to people.  And as you discuss in find out who your group members are.  You can select on the Canvas People tab and uh, understand that each team will be guided by a tutor.  Right. So there are  groups, but each group will be guided by your tutor.  And each tutor will guide, uh, six groups.  So if you do the math, basically roughly  students, uh, within uh, of  groups,  six members per group, and all of you will be at least under one tutor.  And we have three tutors, so you'll see their projected on. So a Google form will be shared to enter your group details.  So at the end afterwards I will I will publish the Google form on canvas.  And then you can click on it and start entering the details which should match the canvas tab.  Um, instructors will make the final decision of your group members and project.  So the form the phone will will list down.  Well, I can show you the form, so I'll do that. Yeah.  So here's the here's the people tab. Right. This is the people that. So you can go here and you can project groups.  And here are the names you can select. You can put yourself into the groups ethnic groups.  And here's the form yet to be published but will be published soon.  So here you put in all the names of your group members. Here you select which is your group  to .  So it should match the Canvas People tab.  Your group number in the canvas will tab and you will have your first choice, second choice, third choice, and fourth choice of research projects.  The research projects have been predetermined. We have eight research projects.  List down. What are your priorities?  Hopefully you get what you want, but you have to make sure that the load is also equally divided between, uh, the two of us.  So just list down your priorities and then you will be assigned afterwards.  Uh, by basically confirmed by week three. Uh, okay.  The it's actually in the, uh, if you go to modules, this basically here, uh,  at the end of the course, I will publish it and you get to see it directly.  Okay. And then, uh, by week three, whoever who is not yet in a group, by then, you will be assigned to be in a group.  Um, you know, uh, we try our best to match six people per group.  As mentioned earlier, uh, the first half of the semester will focus more on the literature review site.  The second half will focus on conducting research, write implementation and findings, so on and so forth.  So hope you guys have fun. It's going to be a nice ride I think going through the projects.  And now I will pass the, uh, slides to the tutors.  We have three tutors, Brenda, Gerard and Tamale. And you know, the slides are arranged in this in this order.  So yeah, you guys can come and share your research projects.  Please pay attention, see what's going on, and if there's some time afterwards, you can also try and grab them and chat with them a little bit.  Otherwise next week you can also chat with them. Yeah. Thank you.  Uh. Hi everyone. Uh, my name is Brenda.  Um, I did my bachelors degree in computer science.  That's a translation, but it was a little bit more of engineering part.  Uh, I studied in Mexico. So when you translate things, sometimes things get weird.  Uh, then I worked five years at IBM. I was a Java developer and a delivery manager, so I'm very industry based.  And then I realised I would like to do a lot of, like, research and cool things.  And so I am now doing my master's degree in computer science.  I am very interested in human computer interaction.  That's why I'm here. And I've been, um, uh, graduate teacher assistant for the past, uh, year.  And so I have two topics. Um, one the first one is, uh, intuitive interactions in AR, VR, mixed reality.  I opened it to, uh, everything that's just, uh, mixed reality or XR, because I am aware that not everyone has a headset and we might not have.  Like, uh, available to to share for all the groups for the  students that we have.  So use your phones. Uh, or be creative.  Uh, it's very broad topic. I'm glad. Um, I talk a little bit about what proprioception is.  So basically, I am very, uh, interested in what our body, uh, ability to perceive its own movements in a D virtual space.  Um, how we relate to the environment and how do we interact with it?  Uh, we want to, um, create an immersive experience.  And to achieve this, uh, we are recommending to explore sensory elements such as visual and auditory cues,  or even I like maybe adding an agent that will interact with you in that way to space.  And so this can elicit the user's curiosity and encourage exploration.  Uh, a lot of the times when we have like spaces like mirror, for example, people are like, okay, but what do I do here?  You get into the virtual space and then you, you don't do anything with it.  So what can be more intuitive than just putting like a set of maybe instructions  or having the the user to click on next to understand what they can do?  Um, for this, uh, we have uh, current project that's called embodiment in VR.  Some of you, uh, might have already seen it in a demo of two, four, five last year.  So for that one, uh, we can give you access to, to to that project.  It's about, uh, embodiment and dancing.  So that would have some motion capture, uh, elements that you can use and some, um, uh, visual effects.  Um, uh, and if not, you can also, uh, leverage the, um, a spatial IO that's, uh,  with the metaverse, uh, using unity, you can create your, your own virtual space.  There's already the, um, the template where it has a lot of examples.  So you can just download that, try to play with that,  and then create your own virtual space and try to research how you can create intuitive interactions in that virtual space.  And then the other topic, uh, it's understanding intentions through sensory feedback.  So this one relates more in that virtual space, more about, uh, visual effects, um,  how something that looks may be fiery or how maybe something looks, uh, like metal or, um, maybe how something looks water.  How does that make the user feel? And there's a lot of research on how you relate yourself to your avatar in that D virtual space,  how you see yourself and how do you understand, okay, that person that is there, it's me.  And how do I relate with that? There's, um, a theory, uh, Laban's theory of effort, which provides a system to understand why we do a movement,  what qualities and intentions behind the actions we do have.  So when you talk about embodiment, it doesn't mean that it only has to do with your whole body.  It also means like everything that you do, you do not have to plan to do so.  For example, uh, me telling you this, I didn't really just like learn it by heart and I am doing this intentions that I'm doing.  It's just coming from some place in my brain that I don't know what it is.  So there's an intention and. Well, uh, we want to maybe understand how those intentions, um, uh, are affected by the, uh, uh,  visually or auditory or sensory feedback through those avatars in movement based interactions.  Uh, and for this, uh, you if you are interested in doing, um, some visual effects or some modelling part or some,  uh, assets, you can or if not, you can use the free ones, uh, you can find a lot more.  I just, um, included one example.  And if not, there's also in spatial audio, uh, of the metaverse, you can create your custom avatars also with the template in unity.  So you can create, uh, other skins or other effects that you can add, and then maybe you can see how people react to that.  How do they relate to that? How does that make them feel, how immersive it is, all the other things that Pi talked about, uh, measuring.  So, um, those are the two topics there are very broad.  So the sky's the limit of the virtual space you want to create.  Thank you. Brenda. Okay, Mike. Test. Can you guys hear me? Yeah.  I always do this to make sure that my voice is loud enough. So I'm just going to make this, um, full screen.  Thank you. Microsoft. I don't need to try that. Okay, so do we allow this whole general advice?  Okay. All right.  Microsoft, I don't need your nonsense. Sorry. Okay. Um, the sponsoring item.  So. Yep. That's me. My name is Jared. So I just got my page.  Just started my page. See, this is I'm one year older than you guys and putting this away.  So I was previously taking software engineering, but I guess I jumped ship and went to the faculty of science.  And I said, I'm still holding a contract with the Faculty of Engineering right now, a research assistant for them.  Uh, I was also involved with security stuff and a bit of HCI.  That's when cave had. Currently I'm the president of the University of Auckland Linux User Group.  So we just started this year. If you guys are interested in Linux at all and you're in this class, okay, you can join us.  Legacy. That's the website legacy.  And we give you tips about Linux and not just Linux. Really, if you're a mixer like I am.  Also, uh, you can also learn some tips about the terminal and whatnot.  Okay. So, uh, that's my LinkedIn. You can scan it, you can connect with me and that's my GitHub.  You can follow me. I'll follow you back. That's a guarantee. All right.  Enough marketing. So, uh, how to go next okay.  Yep. I have four topics. Proposed JJ.  You'll see this also reflected on canvas.  Uh, but you can also scan the QR codes that will, uh, put you to a Google Docs and give you more information about each topic.  So the first two topics are related to my PhD research, which is related to mail and uh, phishing.  So when I say phishing, I don't mean going to a river and getting some food for your self, right?  I mean phishing that lands into your email client. Okay.  So uh, basically I'm looking into putting a chat bot that you integrate with a mail client, and the idea is that this chat bot will pop up, right.  And maybe educate the users about any phishing attempts.  Uh, so has anyone heard of Clippy? Is anyone old enough to have heard of Clippy?  One person. Nice. Okay, I feel very old now. Unbelievable.  So I'm just one year older than you are. Uh, that's also that is Microsoft Office assistant.  So if you look it up, it's one of the early examples of and of a co-pilots right.  Today it's being abused everywhere GitHub co-pilot. Windows co-pilot, Microsoft co-pilot.  Whatever co-pilot out there. You know, uh, but Clippy is one of those early ones.  You interact with Clippy not by, uh, typing in a chat, but it does give you a chat like interface at the bottom, right.  And so you click in such and text bubbles to tell Clippy, hey, okay, this is my response.  I don't need your assistance or yes, I do need your assistance.  Can you help me improve my grammar and so on and so forth. So that has proven to be annoying.  And there's a lot of research about that. Uh, but I'm also wanting to put this chat bot into the mail client.  Uh, the idea is that it will pop up, uh, when maybe this some phishing mail phishing, you know, like interaction.  So maybe the user's about to click on the link, maybe they're about to hover on a link,  or maybe they just opened this phishing, uh, this, you know, phishing email.  Right. And so that chat bot will show up and educate the user.  So one of the findings is that one of the, um, thinking anyway,  is that learning is maximised when you show the user on the spot, like, you know, very close to the event.  So when they're about to hover, that's when like, you know, your neurones are running and then this chat bot shows up.  It can be annoying, but it's also it also maximises learning.  So we're trying to strike the balance. When to show that chat bots uh, to maximise the learning but to minimise the annoyance.  Right. And some of the suggested, uh ways you can do this is okay.  So you can pick an existing mail client. So if the QR code I suggested going and doing round but but you can of course pick easier ones.  Maybe there's some react based client there as well. So I don't mind doing react like I mean that's my background.  Uh, you're going to add a chat bot functionality to that mail client and you're going to perform what's called a repeated measures design.  So maybe one with and without the chat bot intervention.  And if you're going to do this on multiple groups, you also have to um,  maybe you want to consider varying the like the order because there's the audit.  Stats you're going to learn sometime as you go into the literature in HCI.  And so the expected outcomes is that you have a modified mail client.  Uh, you can come up with one yourself or you can adopt one from the open source world.  That includes the chat bot interface for fishing assistance.  And you also come up with maybe a list of recommendations as to when to invoke the chat bot interface.  And there's going to be different demographics. Okay. So you have university students.  Maybe they have different proficiencies with emails. Uh, some of them already know what phishing emails are.  Some of them don't know. So maybe for each demographic, for each target you have different, you know, times when you invoke this chat bot.  So that's J one if you have any more questions, of course we can talk at the end of the lecture, but that's the QR code for more details.  And then moving on to J two. So it's kind of related to J one, except that we're not using chat bot this time.  So we're using visual overlays. So there are different ways to highlight parts of an email.  You can highlight that. You can maybe have an animated arrow, you know that swims to the pot.  Um, or you it can be just as simple as say, you know, a yellow rectangle with a black exclamation mark in the middle.  So there's a bunch of these. So these are called fishing cues.  Uh, many various fishing cues you can attempt.  And the goal is to find the most effective and least annoying fishing cues that grabs the user's attention, right, without causing frustration.  So same thing with the J one. You have to pick an existing mail client adopted.  Uh, and you can maybe add the ability to programmatically emphasise on specific parts of the mail client.  So your code at the end of the day, I could probably use it for my PhD, uh, just modified.  And some of the things you're going to perform repeated measures design as well.  And uh, like earlier and maybe you can consider this.  So you'll learn this in the later part of the course the system usability scale size and or liquid scale.  So if you know those things from a scale of  to , you know, uh, or maybe, uh, you agree with this,  you very much agree with this, or you slightly agree with this, or you slightly disagree.  Disagree very much disagree. So that's the liquid scale over the um expected outcome.  So also modified email but this time with the ability to highlight parts of the email.  So just so you know, the two types of emails in the world plain text and HTML.  Sorry I'm covering. So maybe you're can if it's a HTML based email, maybe you'll modify the CSS of the HTML email bits,  add the highlights or add some box whatever, and maybe recommendations as to what visual overlays to show under which situations.  And again, like I alluded to earlier for which demographics. So for experience users or for non experience use a threat.  Again QR code is there for more details, but at the same time you'll find everything on canvas.  And. Okay. Moving on. Uh, J and J, I should acknowledge Robert Moore for this.  And I should also acknowledge him because I did my honours research project under him.  Uh, and it's related to VR and psycho physiological metrics.  The idea is that you'll use VR, uh, and then maybe assess the task performance of some simulated task in the VR environment.  But at the same time, you're going to link that to maybe the physiological metrics.  So for example, um, one of those one of the research I've undertaken back in my analysis is that, uh,  we put people in this risky construction environment and over time, as they do the task, uh, they get risk, habituation.  So they start being less and less and less aware of those, uh, dangers in the construction site.  Like a crane is going to unload something. So initially they'll be looking up, but after a few times of that happening, they'll simply forget.  And soon the the load on the crane hits the head and they die.  The virtual environment. Now. So we can simulate something similar like that.  Uh, of course it doesn't have to be in construction. It can be in other places as well.  So, yeah, uh, can be public speaking, for example.  And then you're going to be linking and you're going to see if, okay,  is there some correlation with the way they're performing in this VR environment with maybe the physiological metrics.  So again, things like your heart rate, your electrodermal activity also known as your skin conductance uh,  heart rate variability, which is different to heart rate, you know, pupil size.  If you're going to use this VR headsets that we have available with eye tracking capabilities.  And yeah. So you're going to you're going to basically link VR task performance and the human physiological performance.  Um yeah. So this one's a little different. So in terms of these suggested methodologies.  So you might perform what we call one group pre-test post-test design.  Uh, the idea is that you're going to collect some baseline physiological and psychological metrics.  So again these are some words that Pyle handed to you guys earlier.  Um what is the default heart rate when they're not doing anything.  Versus what's the heart rate inside the VR environment. And maybe maybe the skin conductance.  So if are at rest what are they like. Are they you know sweaty.  Are they not. Are they joy. And if they're in the VR environment, maybe this tub becoming more excited and so become more sweaty, right.  So, uh, you do this with one group pre-test post-test, and you take the baseline for pre-test,  the activities in the VR environment, post-test and, uh, j for sorry, I keep coughing.  So this one also uses VR, but we are aware of conflicts around the world, so I don't want to name them because there's a lot of them.  One recently in, uh, you know, you or I don't know the state but America.  Right. And so you have the idea is that, okay, maybe you support one group and so you're considered in group if you're supporting like,  say one side of that and then the who you perceive as enemies are going to be outgroups.  Right. Um, there has been previous research by um academics, Hassler and other researchers.  Uh, they found that if you're immersed in the VR environment, from the outgroups perspective,  you actually start sympathising with what the your so-called enemies are feeling and they start seeing both sides of the story.  So the in-group, they start being less sympathetic to them.  And with the VR environment, they found out, okay, it's not necessarily because, uh,  they find that the actions are bad, but more so they start to target the in-group soldiers, for example.  Hey there, my people are actually doing a bad thing. And so this hopes to bring in more empathy to the research participants.  Right? And, you know, take away that polarisation that we are experiencing today.  And so the idea is to, uh, tap into the potential of VR to create an effective scenario,  something that adjusts your emotions to promote empathy and bring about understanding.  And yeah, so you also probably suggested to collect a baseline, uh, physiological metrics for this.  So what is that default heart rate. Uh, and at the same time you can expand on that, right?  Uh. You collect the baseline, but you can also still do repeated measures.  Design. What if you showed them a video instead?  Uh, well, their heart rate changes much. Versus if you immersed them in a VR environment.  And you can also probably put, um, some surveys there.  Ask and see. Okay. I feel more strongly in the VR versus in the video.  Or the video is actually more effective than the VR environment.  So um, yep. Bunch of options for this one.  So again, that's the QR code. Uh, everything is on canvas.  So with that said, we have Tamil, uh, the third tier, who has left us a video.  So he's in Adelaide right now. Thank you, Brenda Jarrett.  Um, I got some more. I guess you could throw in a little bit.  Okay. So, um. Yeah, some of us on our own, but I have a wonderfully recorded video from him, so we'll play that out.  The slide contents are on canvas, but [INAUDIBLE] share it with you guys.  Hi, everyone. Um, um, I am Human-centered, a researcher, currently a student at the Computing Lab in Oakland.  My background is, uh, artificial intelligence, machine learning, and human centred design.  My key goal of my research journey is to create an empathetic, intelligent, uh, immersive and back-to-back society in that vein.  And being with, uh, I'll be your guide for this advanced HCI course.  Today, I'm excited to introduce you to two innovative projects that are part of this course.  You can choose. Uh, one is Emotional Interactive Systems for Mental Health Support, and another one,  enhancing group cognition in remote meetings using and mediated feedback.  These projects particularly focus on leveraging my friends HCI concepts and machine learning to address challenges.  Let's delve into details of each project. The first project is emulation of air interactive systems for mental health support.  This project aims to enhance mental health support through advanced machine learning technologies,  and we particularly leverage a mono data set which captures detailed emotions and causes  from tweets around the globe of each participant or patient to develop intelligent models.  And these models will help us to understand and identify different emotions and determine the causes for these emotions.  And we can create and design a user friendly interface for these, uh, models and, uh,  help on offer support, uh, using and or leveraging the emotion that caused by each individual tweet.  The main objective of this project is to understand and identify, uh,  different emotions, develop and deploy any ER, models to recognise these emotions,  and try to anticipate future emotional state using the current twin by, uh,  applying a predictive algorithm and provide personalised advice and support to these users.  The main methodology will be utilising is to develop, uh, or train a pre-existing, uh machine learning model using mono data,  set design and prototype uh initiative and user friendly interfaces for communication and interaction,  and use this study to apply uh to validate our predictive algorithm and its accuracy, its helpfulness, and its user satisfaction from people.  The expected outcomes would be, uh, development and deployment of existing AI models and designing, prototyping and testing user interface.  Engage in user studies and data analysis to refine the system.  Uh, this, uh, I'm particularly interested in how this particular project can be applied to users, uh, in University of Auckland.  We, uh, anything that happens inside the University of Auckland communication channel.  And how can we offer support in using this particular, uh, data by leveraging this particular dataset?  The second project is enhancing cognition in remote meetings using AI mediated feedback.  So group cognition in particular. I'm, uh, interested in how when two people have more people collaborating together in a remote meeting,  how can we leverage them to understand or improve the performance that otherwise not possible during the meetings?  So we'll be employing, uh, the, uh, main objective of this project is to employ a cognitive or affective mediated agents to provide  intelligent and real time feedback on participants cognitive load and affective state during the meeting.  This particularly focuses on real time processing of behavioural data, adapting this, uh,  adapting this behavioural data to a feedback mechanism providing personalised uh to uh mechanism feedback mechanism to engage your defence,  multimodal interaction analysis and dynamically adjusting the task difficulty during this particular remote meeting task.  The main objective is to enhance group performance and satisfaction in remote meetings.  It would be particular investigating the impact of cognitive affective feedback on group dynamics,  and develop an adaptive feedback mechanism using interactive visual analytics tool.  Uh, the main methodology would be utilising is to real time processing of facial expression, gesture time, uh,  to gather the data on cognitive affective state of users using machine learning and personalising this uh uh,  what's my personalising the behavioural data based on individual differences.  Uh, then following that, I will do a multimodal approach for, uh, future expression, gestures and audio data or sentimental, uh,  analysis and utilise this sentiment analysis to adjust the task difficulty based to match state cognitive state and provide,  uh, interact visual analytics tool for self-awareness and effective practices.  And the expected outcome for this project is development and training of these agents that can help, um, improve their group performance,  time, data processing and feedback mechanism design collaboration in creation of this interactive visual analytics tool and these projects.  Uh, I chose to help you with this, uh,  leveraging high density content to create impactful solutions for mental health support and remote meeting efficiency.
