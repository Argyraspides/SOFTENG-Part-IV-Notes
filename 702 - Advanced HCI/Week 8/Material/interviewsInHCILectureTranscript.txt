Good morning. Good morning.So, uh, today we will be getting into some qualitative analysis, which is something that perhaps you might be doing for your project.Uh, whether you are or not, it fits in to what we were discussing last week around different research approaches,with quantitative being one major approach in qualitative being another.Uh, before we dive in, I was wondering if there were any questions about the course.All good. All right. So let's contemplate this quote by Henry Ford.If I had asked people what they wanted, they would have said faster horses.How does this apply to you?And to the kind of digital innovation that you might want to do?And he guesses customers. They don't know what part of me the customers, don't know what they want.So yeah, yeah, customers don't know what they want or what they need,but they sort of there's like in a way, what this is saying is that they want they know they want speed.But instead of being able to come up with the idea of a car, they're just used to horses.They have no idea about cars. And so they're like, okay, the proposed solution for me to get speed is a faster horse.And so. The idea that I'm putting forth here is that your users, your interviewees, they know something about their lives.You know, they're experts on their own lives. They can say, if you say, what do you need from this technology?What's your user need? They may have a they have a sense of what they need, but they can't do the design for you.And so the, the the basic, uh,practical implication of what I'm saying is if you ask your users what feature should I design or what feature do you need,they'll tell you faster horses, because that's all.Because they only know what they know, which are horses or whatever apps they're already using,and they'll give you a solution based on that, even though they do have a sense of what they need.So the idea isn't to blame the users for not being smart enough.They're not designers and software engineers and computer scientists, so you can't blame them for that.It also means that you can't lean on them to do the designing for you.That's going to be your job.And so the kinds of questions that you can ask are not, what features should I design for you or design me a feature that you'll use.But you need to understand their perspective of like, you know what could be better?What what could what problem are you having that this feature could solve.And then you do the designing. So it's about coming halfway where they describe to you what problem they're having.And then you're the one to do the design implications.If you ask them to do the design implications, and then you design what they tell you to.I'm not sure that it's going to be a good choice. All right.Continuing on. So today we're going to start off with interviews.And then we're going to do a little bit of practical interviewing where you're going to be asking each other some questions.We're going to do some qualitative analysis then finish off with ethics.So there's a lot of fun stuff to get through. So first of all critique I don't know if you're awake enough for this, but it's so fun.Maybe it'll wake you up. So after a user take the scenario, a user has used the system X for five minutes and we ask them, was that easy to use?Good question or bad question? What's wrong with this question?Yes. No. It's just like yes or no.Correct. So it's not open ended. It's just a yes or no.Yes. Pardon me if it's not sorry, but if I want to.I'm sorry. I'm not understanding exactly what you're saying. Which makes them entrance using figures.Oh, what's the metric? Yeah, yeah. So currently, we don't know what it could be if it's like a quantitative metric or qualitative metric.Specific. It's not specific. That's not necessary.Like it? Not necessarily a problem because it lets them decide a little bit what may have come up as easy or hard.Is it biased in any way? Do you if if I were asking you.I had given you a system that I had made. And then I said, was that easy to use?Would you feel forced towards a certain answer?Yeah, yeah, if I had asked. Was that what what was difficult about that?You might answer something completely different. Let's say.They use Systemax. And this is a real situation, real experience that I've had.Uh, and let's let's give you, let's put it in an example that you might understand.So let's say you have to do something like print your schedule or print a transcript from Esso.So you have this task that you have to complete. And let's say that it was so badly made and so confusing.Then at the end you thought you had printed it, but then you hadn't realised yet.It hadn't worked. And so if in this intermediate stage where you thought you had it done,but it didn't actually work, if I asked you was that easy to use, what would you say?Yes. Yeah. Because you think you've done it. Your perception is like, oh, yeah.Great. And so I'm not saying we shouldn't have the perception of the user.If your perception of the system was like, hey, that was great, I felt I felt good during that.That's something that's good to know. But this A is not giving you objective data of whether the system was easy to use.We don't know whether you accomplish the task or not based on this answer.And B, it's always going to be the user perception which is useful but not the whatcould be considered the ground truth of whether the system was easy to use.Okay. You've used system X for five minutes.What do you prefer the new way to do X?Say print your transcript or, uh, system Y that you usually use.So you're asking the user to compare the new system they've used with five minutes for five minutes with the system that they've used for a long,long time. So.So critique this the same critique that you gave last time.Do you recall? Yeah. Two restrictive. So the answer to this would be x or y.And we have no idea if you just asked them that what the reason would be, which I think could be pretty important to know.Anything else. Size.Thanks. Okay. Sorry. There's just two people answering at once.Um, and so, uh, one answer here was that it's biased because there's, uh, a big difference in familiarity.So this is a problem that actually we come up with a lot in HCI and especially all around innovation,that you might have something new and people have been using something else for a long time.And so it's it's a very tricky problem to compare them. So I think that that's a challenge that we have.And then over in the corner, uh, you. Given enough time us to.Here is why they all like you. Yeah.Yeah. So.So there's this sort of deep level problem of comparing something new with something old, but then also like, how much time should we give them?Five minutes is certainly probably not enough.Okay. So. Do you create better?Whatever it is, whatever the task is the user is trying to do on system X compared to Y.Same critique. Gloom.Oh, no. I mean this. Because you're asking them.If it's. Yeah.So there's like, uh, there's the fact that this answer might be yes or no.Yeah. And that's not great because we have no idea about some really interesting,mysterious things like what do they define as better that that would be really good to know.So so yeah, we want to know what they might define as better.And then all of the other critiques around the fact that it's a bit of an unfair comparison that we have to deal with.And then how would you change system X so that you could use it every day?Huh? Yes. Uh.That's good. But it's asking them what they would change.So. So it's not too restrictive.But the answer is faster horses. So we have to be careful about that.And there's also this big hypothetical situation.What alarm could I design for you to make sure that you got eight hours of sleep every night?I don't know how well y'all are sleeping, but I feel like nobody's getting eight hours of sleep every night.And it's not the design of an alarm that would change that.And so just the presumption that there could be a change to the system that would hypothetically make them use it every day.There's a whole levels of presumption and hypotheticals here.Not that we don't want to know. Like if you have a Start-Up and you're wanting someone to use it every day,like that's a real problem that you're facing, and that's something that you do want to find an answer for.However, the right way to find out the answer is to not ask a hypothetical question like this.Instead, it would be to do a lot of background research.What? Who's getting eight hours of sleep? How are they doing it?What systems are in place? You know what? When have they, like, fallen off track and gotten back on track?All of those like long term behavioural like understanding and observations and all of that use of features.That is what will help you design the next feature.Any questions? Yes, sir.Could it be related to that instead of asking him like, oh, like, what did you think about our system?Would it be to like ask them about system? Why did they exist?Like trying to use like. Yes. So you are in okay?Yeah. Absolutely. Absolutely. So usually, you know, uh, even when we're innovating, there's some sort of relationships to some existing system.And so to understand how is that system successful.Is really good knowledge. Yeah. Okay.So today we will think about which questions are people.Our interviewees are you or users can answer accurately and what they can't.Uh, we'll look at different forms of interviews and understand common techniques.And the implications for design. So we've already started all of that.And so here's here's another outline.Basically there are three kinds of interviews structured unstructured,semi-structured and some various techniques where we'll learn the definition of these critical task based artefact based.So who plays chess.Hands up. Okay, so the chess players in the room will see something fundamentally different than the non chess players.The chess players in the room will see who's winning.They'll see what the next best play is.They'll know what are legal moves and what are not legal moves.And if I took this away and asked where are some of the where are some of the pawns or where some of the pieces between thechess players will be able to give a better answer because they'll just perceive it and understand it in a different way,uh, so that they're sort of remembering fewer things. Whereas like a non chess player, they'll be like, oh, there's the horse one.It's about over there. There's the other one. And so they're remembering things in a whole different way, seeing it in a whole different way.So every time you look at an interface, you're a chess expert and you see things in a fundamentally different way than what your users might see.And so this is quite interesting. And part of your role in a user research HCI role is under is having that empathy for like what do they see.And it's fundamentally difficult for you because you'll be chunking things and seeing thingsand understanding things in a much more complex way with your knowledge of technology.And you just have to take, uh, take stock,that it will be fundamentally hard for you to achieve empathy because of your expertise in IT, software and technology.But what you can do and what the point of this slide is,is for you to to know that expertise changes perception and it changes how you look at things.It changes. You know how you understand things. Of course. Uh, so.That's that's the point here that you, going forward, will always be the chess expert when it comes to looking at any sort of interface,and your users may encounter things in a completely different way.Now, how do we get to being an expert?And so there are some theories around that. Totally fascinating.Uh, and and one is, um, dreyfus's model of knowledge acquisition.So, uh, we'll tell you about it in terms of driving.So the first time, if I don't know if you remember, if you've learned to drive, if you remember learning how to drive.You get into a car and you're like, okay, what are the rules of the road?You know what? What windows am I supposed to look at? Uh, you know how, uh, how to how do I turn the wheel to get around a corner?So that's it's so much that you're learning. You're learning at the perceptual level.You're learning motor control. You're learning knowledge of the road.And so clearly you're under a lot of cognitive load.Like there's a lot going on there. And you're rewiring your brain with that kind of learning.Now the first couple of times you turn around a corner, you'll feel out, you know, how much am I turning?Did that work out? Did that not work out? And so you're getting feedback all the time of like, is this going well?Is this not going well? This is how I learned to drive. This is how I don't learn to turn that way.And so in all sorts of domains in our lives, we have hundreds of thousands of experiences of turning a wheel around the corner,which is how we know how to turn a wheel around the corner.And so if you're an experienced driver and I ask you, how do you know how much to turn a wheel around a corner,you'll be like, oh, I haven't thought about that for a long time.In fact, I never have to think about it. I just do it. And so that happens a lot with a lot of things in human life.And so the point of this slide is to say, sometimes you might see someone, uh,who knows how to do something like drive a car around a corner and you want to know, hey, how do you do that?Sometimes people don't have access to that kind of information because they've learned it in this really natural way where there's, say,hundreds of thousands of experiences with emotional feedback so that they havethis sort of gut feel that drives them in all sorts of decisions and behaviour.And so you might want to learn about it, but they can't even explain it to you because it's not like a declarative knowledge in their brain.Hence, there are some questions that are hard to ask users, even if they're experts.So a little rundown of the five stages of learning.So as a novice so we can apply this to driving, we can apply this to chess.We can play a whole bunch of other things. You're first learning the rules, then you're an advanced beginner.You're you're learning more context of how to apply the rules.Uh, and again, you're having these hundreds of thousands of experiences that give you feedback,that give you this gut reaction, this intuition that's developing.How do I make decisions? What feels right? What feels wrong?Uh, and then you become competent, proficient, you know, with reinforcement of all this,hundreds of thousands of experiences, and then you become an expert.So same thing with doctors when you come in and you have like this sort of series of symptoms and they have to feel out.What diagnosis do I think this is.And again, this may come from hundreds or thousands of visits with patients and seeing different symptomatic cases where they have a gut feel.I believe it's this. And the gut feel comes from rules, but it's not actually an application of rules.So hence users. And so, you know, many people know how to drive.Many people know how to play chess.And hence we have these kinds of emotional gut intuition that guides a lot of our behaviour, a lot of the time difficult to ask questions about.Okay. So other implications emotions are core part of reasoning, especially expert reasoning that you have implicit knowledge.So again something like riding a bicycle.Uh but there are all sorts of other things like, you know, I'm choosing some examples that are more like say motor motor based.But we can we can use the chess board as another example, where you have some implicit knowledge, where you give gut feels for things,and you can't necessarily explicitly describe any rule that you're following because you're not actually following a rule.So users, if you ask them, hey, you just did that chess move, why did you do it?Are they going to give you an answer? If somebody did a chess move and then you asked them, why did you do that move?Do you think they'll give you an answer? I think so because there's a lot of social rules of how you behave with other people.And so if somebody is like, hey, you just did a thing, why'd you do it? You'll be like, oh, this is why I did it.Right. And so users won't necessarily stop you from asking questions where they don't know the answer.They'll just try their best all the time.And so you have to be careful, as the interviewer what questions you're asking.Um, and you have to take sometimes a grain of salt that people will try their best, you know, and maybe they'll even believe that they have an answer.And so there's all sorts of interesting psychological experiments about that, uh, where they ask people questions or.Yeah, like they give them, like a caffeine pill and had them read an essay and they're like, how much is this essay provocative?And people who took the caffeine pill are, like, extremely provocative.And they miss attribute their feelings of being provoked to the essay as opposed to the caffeine pill.And so that's just to say that humans are not perfect. We're not always completely aware of our own motivations for our behaviour.And so this is just part of, um, a kind of sociotechnical understanding of humans.So HCI is partly psychology. So we we don't always have accurate answers, but people will certainly try to help you out.And so if you ask them something like why did you do that last chess move?They'll explain to the best of their abilities. It just may not be the truth that they had a gut feeling.So the implication is try not to ask questions where they're not going to know the answer, and they're going to have to make it up on the spot.Okay, so another big situation where this kind of same theme comes up is around making plans.And honestly, this is a big reason why we've moved from the waterfall model of software development to agile.And so we can sort of almost even use agile as, as a bit of a metaphor for what's going on here.So for, for this plans in situated action, let me give you a scenario.I am going to navigate across a river.And so I'm on one side of the river. I'm going to go ahead and navigate to the other side.If you ask me how do you plan to get to the other side?Again I'm going to be a good interviewee.I'm going to give you my plan. This is my plan.I'm going to look at the dashboard, and I'm going to turn every time the dashboard tells me to.And then I'm going to get to the other side. Now if you as a designer, take that plan and go create a new navigational system.I think it may be a failure, because that plan is very different than what will happen in reality.So if you instead observe me crossing the river, uh, with my like, existing system, why?And you see, you know, what situations come up, how I respond to those in the moment.Uh, you know, unexpected gusts of wind push me off course.How do I get back? Those observations will make a much better design requirement for your navigation system compared to the plan.The plan is pretty worthless compared to actually observing how someone crosses the river.But it's really messy and complicated.Oh, the cross like this thing happened, and then this thing happened, and then this thing happened, and you're like, oh, puzzle over this.How does this make a design requirement? That's our job.Because if you just take the anticipated plan that doesn't conform to what actually happens.Does that make sense? All clear. So y'all will never ask for a plan and always will observe from now into the future.Pinky swear. Cool.Thank you. Okay. Moving on.So now we've done a chunk of the lecture on what are good and bad questions to ask.And what do people know? Even what do experts know about their own behaviour?And what do experts not have access to? So now we're moving on into conducting an interview.So, uh, these are the sort of five, uh, sort of main parts.So overview, uh, and this comes from a little bit, uh, prefiguring informed consent.So we start the interview process by always saying this is what it's going to be about, and this is how long it will be.Uh, and usually putting a participant at ease of like this is not evaluating your performance.So with all of your, all of your projects, uh,this is quite relevant because if you're designing a new interface and you get people to try it out to your pilot participants,you don't want them to think that you're testing them.And people sort of feel just generally sort of on the spot like, oh, I'm not going to do well enough.You're going to think I'm stupid or something. And so this we want to hedge that and make sure that we don't put the participant in undue stress.And so we say, hey, anything confusing, anything you don't know how to use.Like that's especially helpful for us to hear about. So notification and consent.So getting more, uh, into the informed consent.So what data will you be collecting? Explain how it might appear publicly.Obtain clear consent in a written or verbal way.And so in in today's module you will see an example of participant information sheets and a consent form.So you will adapt this template for use in your own protocol for your project.So questions? Where to start? Uh, usually some sort of simple things to start.And then, um, if you're understanding, if you're if you're innovating on system X,as we learned earlier in the lecture, it's good to understand how they use system X.Mean interview and then thanking the participant, giving them a chance to say something in the end.So interview structures. So basically there are three kinds.And there's sort of one slide per per kind here.So I think it's it's pretty it's pretty clear pretty pretty easy.So unstructured just means that you're trying to ask for a particular topic and you let the interviewee lead the way.You say how how do you commute to campus?And then you let them tell you about all of their like, buddies who give them a ride sometimes.And when they use the trains, public transport, you know, what they did last year, what they did list this year.You let them lead the way, tell you everything you need to know, uh, about how they commute.Because if you had, let's say, a structured interview, what is the frequency that you use the Auckland Transport app,you'll clearly miss out on a lot of stories that they could tell about how they're commuting,but you could ask  people about the frequency of using the app and then get a sense of how frequently the app is used.So again, different kinds of interview structures for different purposes.So unstructured is rich. It's not possible to replicate.And the interviewee tells you what's important, which is good when you're just learning about a field and then structured is tightly scripted.You know, everybody gets the same questions in the same order.And then you can more compare across people. So this is usually good to do when you know the important questions to ask.You're like, oh, I should ask about their, uh, like group commuting arrangements.I should ask about, you know what, local transport, public transport is available in their neighbourhood.I should ask about this. I should ask about that.So if you have good knowledge of the domain, usually structured is fine and then semi-structured is in the middle.You can sort of make up follow up questions on the spot if there's anything that you want to learn more about.So again, I think I have pretty much covered these points, um, in terms of the pros and cons.Uh, maybe something else. I'll.I'll mention that's here that I haven't told you about yet, which is the difficulty of figuring out a follow up question in real time,and the fact that the resulting data is, again, pretty messy and all over the place.But yeah, just like being able to make up questions on the spot of like, oh, this is what we should ask about next.Again, that's kind of a type of gut reaction and expertise.And so it is something that may be difficult at first.But after you've done some interviews, like dozens of interviews, you'll sort of get a gut feel for how to guide the conversation.And then oh, oh, unstructured interviews.Oh, I already have a. A case study.So when I was working at Yahoo! Uh, there was I was in a little internal innovation team, sort of like the Google X of Yahoo!It was fun. And we would release apps, uh, unbranded apps to sort of try out to see if we could get a little viral sensation.And so one thing that we one topic that we were interested in is getting understanding local experiences of different places.So let's say you're headed somewhere like Bangkok or Chiang Mai, somewhere in Thailand, and you're like, okay, where do I want to go eat?So probably you want to eat where the locals want to eat.You want to go where the food is delicious, where you have good value.You want that local knowledge. And so we identified this as a kind of travel need that's not currently, uh,sort of there or sort of like there's no, no solution, quote unquote, in the market right now.And so the way so we wanted to see how people who are travelling and who arefinding these local experiences in a sort of foreign place that they're visiting,how do they go about finding it right now? And so I conducted these interviews, and it was really open ended.Tell me about your last trip. Um, what were your favourite moments because you were interested in that local experience.But we wanted to see what else? Like, uh, travellers sought out.And so here are some. This is an example, a screenshot of the actual script that I had in industry and the lab that I was working with.It was it was sort of a Yahoo had acquired Polyvore, which is an app where people like upload outfits that they wear with different items.So if you have, say, a jean jacket,you can look at that jean jacket and then look at all user photos of like different outfits they've worn with the jean jacket.Anyway. Yahoo acquired Polyvore, and a lot of the Polyvore people were in this like, uh, little internal innovation team.So again, these are sort of very open ended.What were your recent travel experiences? How much were you looking to experience popular points versus getting a local experience?So so really open ended questions so that they could tell me what they were doing and how they were doing it.Uh, what do you do on your phone versus your desktop?So we were just trying to get some some.So really like coming back to our initial questions like you want to build system X and people are using system Y.So this is me trying to understand system Y.How do they use Reddit. How do they use Instagram.How do they use at all these things to try to get that local like knowledge about the local experience.And then this is again, a screenshot of one of the reports that I sent lots of bullet points.Um, so we don't need to read this,but it's just something that you can peruse as like the type of findings that you wouldhave from an unstructured or semi-structured between semi and unstructured interview.Uh, some I think some of the, the interesting things where I'll just uh, yeah,go some of the interesting things out here were some of the surprising things, how people used dating apps to get the local experience.So they would go for a dating app, then like just talk to people who are on the dating app being like, oh, you know, where where do you go for dates?What kind of crowd is in this bar? What kind of crowd is in this bar?And that's how they would find out, like what bar they think they would enjoy at this other city.And of course, they use Reddit. They use like hashtags on Instagram to sort of see what, what, what our places.People have taken photos in different places. What photo do I like?Hence I try to go to that spot. Okay.So moving on now to structured interviews. So this is when you know the set of questions they're sort of locked in stone.Uh so you know your topic very well. You can predict the common opinions process is that you can do it with a lot of people.Replicable. You get easy data to analyse.Cons is inflexible, and so you might miss something important if you don't ask a question about that.And then semi-structured sort of in the middle. All good.Well. Yeah. Okay.So now let's, uh, practice a little bit.So imagine that you are thinking about a new travel app that you want to build.And so ask to chat with your neighbour to create two questions for a travel app.So imagine that you want to build a new travel app. You're trying to figure out what it's going to be.And you have to have a couple of questions on your interview script.So go ahead and chat with your neighbour.I'll give you two minutes. Right.Okay. Oh, yeah.Is. Who.Eventually some of these students were these students could be for the..This. There was a small sample of these samples.Yeah. It's.. From the cultural heritage.Okay, so, um, I'm hoping that you have a couple of questions.Uh, I'm not sure if you've had time to revise them for unstructured, structured and semi-structured.Do you need another maybe two minutes for that? Yes.Okay, I'll give you another two minutes to get the different versions now of those questions.Okay. And, uh, we're going to be doing something with some post-it notes.So take a maybe for per person.So yeah, chatting with your peers, having three versions of your question structured, unstructured and semi-structured.Okay. I'm sensing. I'm sensing a quieting of the room.And so can you give me, uh, an example of a question and the three versions?Yes. Uh, suppose we have you asking for some windows or.Do you want to display the restaurants on the map or in the local area with people who are going?Okay. We want to display them according to the ratings of that other restaurant.Choose. You have three. We have a really we have five vinyl ratings.One star, two, three, four five stars. Okay.Do you want to rated according to, uh, you know, displayed and compare with the colour.Suppose somebody is, uh, one that's going to sort of have five ratings.Anyone here displayed in the form of, uh, green colour.On the map, and this is a testament to you.This is a priority. So it will be a green. Then if you supported it then it's orange.And then like this. Okay.So so the the the scenario is around restaurant ratings on maps and their star ratings as well as now a stop light rating that sort of,uh, mixed or somehow sort of superimposed upon the star rating for some easy visualisation.And so what are the three versions of the questions that you could ask about that to your user?Yeah, I didn't attended the lecture. Okay. Okay.So so I think what you've provided, uh, in terms of the scenario, is kind of like the new system that you might want to design,like you have a hunch that people, um, are somewhere local or want to see sort of some restaurant, uh, restaurants in a local area.And they want to choose them based on rating, and they want to do a sort of visual search.So that's sort of your hypothesis. But then we have to ask users questions about that.And so the three kinds of questions would be what we're looking at open ended, more close ended and semi-structured.So given this scenario, does anyone want to offer an example of a question that could make sense?Okay, well, maybe I'll offer an example. So you're assuming the design idea assumes that people look at restaurant locations on maps.And so I think probably you want to ask someone, when was the last time you looked at a map with restaurants on it,just to understand, does it happen once a month?Does it happen every other day? Uh, and you might ask, um.Yeah, I think so. That the sort of frequency, let's say you were like, I have this idea.I think everybody needs this.And so you could have a structured interview question where you're like, how frequently do you look up restaurants on a map?And then you ask everybody that same question, uh, and then a more unstructured would be like, have you ever looked at restaurants on a map?Tell me about the situation. Was it in your own city?Was it in another city and then you really, like, kind of dig deep into why it happened and when it happened and how it happened.And then the semi-structured would be like, how do ratings come into your restaurant choices?Like maybe if there was like high ratings, they would go to different neighbourhoods.And so the ratings might, you know, kind of people might go for that instead of location.And so you're looking at both ratings and location at the same time. And so you might want to ask those questions separately.Any other examples that came up of a question.And then the three versions. Nothing that you want to talk about.You all understand everything perfectly. You can perhaps just give one version of a question.Yes. A good unstructured one. Tell me about your most memorable.With them, in this case to Zeeland. Why did you decide to go with them?Yeah, yeah. Okay, great. So unstructured. Tell me about your most memorable trips in New Zealand.And why did you decide to go there? So yeah, that that would be great.Um. Open ended, like, in the sense that the participant will tell you what memorable means to them and how they figured it out.Uh, the whole the whole way through. So it would give you great ideas for building a new app.Anybody want to provide a structured or semi-structured example?Yes. That was kind of wonderful.Yeah yeah yeah yeah. So that's great. How helpful do you find other travel apps?Well, planning your trip with some sort of scale.Uh, when I was in industry, there was a lot of sort of competitive market analysis of, like, which other apps are doing what?Well, and what's sort of left to be desired, whereas like a hole in the market.Okay. So last chunk of this lecture is how we can include artefacts, prototypes and contexts in our interviews.So, um, usability studies, this is sort of a review from um Soft Edge  or com sorry, three,  or .For those who've taken the undergrad class, it sort of tells you how to do a usability interview.And so I won't really go into depth here,but it's like one kind of interview where basically you have a prototype and you're asking users to like, use it to complete a task.And you're assessing its usefulness in usability, mostly usability.And so usually there's a few people around and you have the prototype there.And so you're interviewing as they're using the prototype,some examples of a facilitator and a user doing some sort of like financial financial prototype.Uh, this is like a mobile phone prototype where there's a camera on top of the mobile phone.And so it looks like this. So you're as in usability, you're sort of recording their interaction.Usually in a usability like industry context, there's like a one way mirror with a room.And so the design team and production team is on one side watching this usability interview sort of like a theatre in a way.The usability and user experience researcher and the user are in the room.Uh, and so and then the feeds, the different camera feeds and screen recording feeds are on the, on the television displays there.Uh, and so this kind of usability interview is a type of interview.This usually with  to  people, uh, where you're getting them to do a handful of tasks for about  minutes.And so that's one kind of thing. Contextual inquiry is another.And that's kind of like a mini ethnography.And so we've sort of, over the course of this hour, talked about how important it is to know how users use the existing system.And that it's hard to just like describe what it is if they're using their gut feelingsand they have a way of doing things that's a little bit messy and unpredictable.And so observing them and interviewing them while they use their existing task is the best way to learn about how users do work.So then you can sort of invent a new thing. So basically an interview in context.So pros the benefits are that you get rich data and it's, it's very, uh, like rooted in the ground truth of their day to day activities.Uh, but then the cons or that it's very time intensive and of course, like not very comparable across people, but I think still still worthwhile.And artefacts and interviews just kind of means like we bring people's stuff to the interview.And so there was the, the, the travel question of like, how do you use other apps a rate the other apps.But something else that you could do is you could say, hey, can you get out your phone and show me,uh, what subreddits you follow and how you've looked at them?To answer this travel question, or can you show me Yelp and how you use Yelp to choose a restaurant?So that that sort of the artefacts just kind of means like having the technologythere so that you can look at the technology and see what they're doing,as well as talk to them about it. So that's the best, best, best, best.Better than talking to them far away from the technology is to have it right there so you can look at it together.And then the critical incident technique is basically seeing maybe the last time they planned a trip.Tell me exactly what you did. And so it helps to sort of again get it in ground truth.So if I were to say ask you on average, when do you video chat with your friends?And you might be like, I never thought about, on average, how I video chat with my friends.Let me think about it now and be a good interviewee and try to answer Danielle's question.And so you'd be like, mhm. I think usually when we're planning something you'd give up.You sort of make up a response. Be a good sport.However, if I said please look on your phone to be like the last video call that you made, when was it?Who was it with? What was it for? You will give me specific information that is actually true.And so then if I ask, a lot of people give me like the last critical incident, I get all this truthful information that's yeah,highly specific and idiosyncratic and all of that stuff and kind of like randomly specific.But if I ask people, oh, give me an average of what you usually do, then that's probably less accurate on in general.Does that make sense? Cool.So yeah, show and tell. Give me a tour. Uh, last time you did something.Excellent, excellent. So a so much to learn in postgraduate courses.I will give you a break very, very soon. This, uh, is an overview of different kinds of methodologies.And so it maps a little bit to, uh, some of, well, some of the different other kinds of approaches that we can take in science.And so. Basically, part of the reason that I give you something like this is to say there's not really right and wrong.There's always like benefits and drawbacks.And so if you want say maximum like, uh, realism.So this is sort of like the critical incident, the field study.Then I go into context, I do contextual inquiry.I see what really happens. It's super real, super authentic.But then it's kind of hard to know, like what do users think about like, you know, you can't answer certain questions that way.And so let's say you have a new design. You might want to test it in a laboratory experiment.And then you would have precision feedback on that design.You could maybe have a causal relationships between, you know, design A and B and like people's reactions.And so in a sense you're creating an artificial environment,but you can still answer certain questions with these methodologies in a way that you wouldn't be able to with this one.Uh, so I think that's basically, um, just like a.A high level discussion of these different kinds of methodologies.And, uh, just so you should know that if you want to look them up and get more information about them,that these are some sort of titles and things that are.Uh, I guess like showing you words.So, for example, the word experiment tends to be in this corner where it's more controlled, whereas the word study is less controlled.Uh, and is more is used in that kind of context.Okay. And so then we have already reworked some of our questions from the beginning.So we will take a five minute break and then come back uh, for doing a little bit of qualitative analysis.So yes, in this break, um, you would be welcome to get five,  or , um, post-it notes, if you can find them, pass them around, please.So yes, I also have some pens here. If you need a pen or pencil and more post-it notes.So everybody should have about  or  post-it notes and a writing implement.All right, let's get started again. And, uh, the first thing is that we'll.We'll go ahead and use these post-it notes.So everyone, uh, should have some post-it notes and a pen.There's more here. If you don't have any. They'll be sort of passed around the room.So we're going to get some data to analyse.So use the two interview questions that you generated on travel,and then get into pairs for a four minute semi-structured interview where you ask questions, uh, that you've created on two questions about travel.And. You get into pairs to discuss and then take at least eight notes.So various things that your interviewee talks about it.The interviewer, uh, should take some little notes of what they're saying on the post-it notes, and then you switch roles.Okay, so I'm starting. The four minute timer.So please, uh.So for folks who've just come in the room, you are grabbing a few post-it notes and a pen, and then you are interviewing each other.You have four minutes, so everyone should get started. And, uh, you're we're going to be analysing this data.And so hence you're interviewing each other and asking some question on some recent travel experiences.So imagine you want to build a travel app. And so you're understanding travel experiences.And then. Of course us.He was. Unable.Tell me about. So if you want to go.Just, you know, you got. Yeah, I should have.If you have. ,.Followers. Yeah, but if you bought them in. Like  years.Be enough. Or something like that.I mean, yeah, it does look like boots on the.So did you go for? Uh. It was like.More than. Just.Just think we actually have a.It comes from the. Message.The same is from God and the church. It's.And for the first time in six second I am.So, you know, because.You know, I was just trying to remember. This was obviously.Something. Great.Mr. Well. As long as there.Yeah. Okay, so one person, um, ask some questions to another.And if you're in groups of  or , that's totally fine. Uh, we're going to have another four minute turn.So something that I hope you're doing is sort of planning a question.And then on the spot, sort of thinking up what could be a follow up question.And so that's one of the experiences that you, uh,an opportunity that you're having now is with the semi-structured interview is to give a planned question.And then based on the answer, you make up a follow up question to be able to uncover more interesting material about travel apps.Okay. So I'm going to give you another four minutes for someone else to have a turn as the interviewer and the interviewee.Comes. The States.Comparison is an option. We did this outside.You. It's like. It's like this crisis in the.It's because feel like. Yeah.You. I thought it was a combination of there's like ten reviews, you know, five star slash descriptions.That's right. Generally I try. About my study skills.So many of us. From the st.Unfortunately, since that I mean so much. Not so much.I want thank you for. Your understanding.It was more business like they. .Conflicts of interest. This is what's interesting. Because you know.So. Do is I might have.Of course. I go to the show?Yeah. With.I just. Walking through the house.Um, actually, yeah, I do find. Sure.I disagree with you.That one. This issue and.Okay. We're back. Oh.Um. Okay, great. So now you have, uh, some interview data.And, uh, the point of the next chunk of lecture is to introduce you to some basic concepts inanalysing qualitative data and to be able to practice with some of what you just generated.So some of the three main approaches that we'll, we'll sort of talk about what is qualitative data.And then three main approaches affinity diagramming grounded theory and semantic analysis.Okay. So what is qualitative data. So clearly the verbal data that you just discussed.Other things that could come up. So observational data.So let's see. Let's say you asked someone how did you plan your last trip.And they went to some sort of subreddit so you could see like you know, you could ask them what what search.What did you search for? You know, show me what message you found that you that had good information.Show me how you followed up with it.And so all of that visual data in that story is part of a qualitative data that you would then have as part of your qualitative data set.Uh, if they, you know, smiled and laughed at a particular time, you know,that could be a moment of like, oh, there was true delight around that experience.So that any sort of images, text, all that stuff can be combined in qualitative data analysis.So something that's kind of funny with this, with you as a group of students, is that often with qualitative data, people talk about coding the data.And so clearly it's a different kind of coding than you will be used to.And what it means is that people try to label and categorise parts of data sets.So different kind of coding. Uh, and so it might be that you're like, wow, is this really science?This seems a little messy. There's going to be some pictures, some stories, some observations, like how do I make sense of it all to try to tell,uh, you know, something rigorous about what I found in this scientific endeavour.Uh, well, there are some metrics that help achieve rigour.And so this is the concept of, uh, saturation and, uh, and having a reference to method.And so you're not just like making it up as you go along, but you're like, oh, other people have analysed data in this way before.So hence it's a reference method and not something I made up.Second, we'll look at the definition of saturation.And third emerging is positionality.And so it may be like that you with your knowledge set a sort of lead the interview down this path.And if you say like oh, this is these are some of the reasons why like who I am,I have this kind of training and I ask these kinds of questions, makes things a little bit less biased.It makes us understand how it came about. And so rigour in qualitative analysis.So saturation is when you have stopped learning things in a qualitative capacity.So you just interviewed each other about travel.If you then interviewed  other students, at some point you would start hearing the same stories and similar themes over and over again.Like, this is how I travel. These are my favourite points. And so at a certain point you can say I've achieved saturation.I've heard mostly what stories are to be told with University of Auckland student body on travel.So saturation is an important concept in qualitative data analysis,which means that you've achieved a certain level of completeness and that's rigour.So you can then stop doing data collection or stop analysing data because you've achieved saturation.And then, uh, of course there's a lot of debate around these concepts.And so saturation and how exactly to define it continues to be debated.So forms of coding qualitative data could be identifying categories.So let's say with travel you could have like information gathering or.Moments of delight.And then as you categorise, say, those different stories, you group them together and then you see sort of the variance that you have.So axial coding is fleshing out subcategories, information searching on say food apps or information searching on attractions or on accommodation.You could potentially have those kinds of subcategories. And then if you have a particular theoretical scheme or a particular question.So let's say you're interested in how people choose restaurants and a maps based view.Then you would pay particular attention to all the stories and all the information related to that.And you would selectively code things to really focus in on a sub area.So that's fine too. So affinity diagramming the important thing is what is the definition of the word affinity.And that gives the key to what this means.So affinity is a bit of a weird word. We don't really talk about the definition.Anybody want to share any ideas of how to define the word affinity?No worries, no worries. I would also hesitate.So affinity kind of means like going towards something or like liking something in casual language.I have an affinity for frozen yoghurt.People don't really use it very often. What it means is like affinity, sameness, similarity.And so basically the affinity diagramming means similar things.Similar note similar stories go together. So you've talked to, say, three people on their travel apps.Maybe two people tell you how they use hashtags to like find travel information and you group those things together.And so the idea with affinity is that you don't always know how things are going to be similar.So maybe at first you're like, oh, I should I group the hashtags together?Or actually, maybe it's more about the images and finding images.And so then you group stories that are about images, or maybe you group stories that are about food.There's infinite ways that things can be similar.So it is our powers of being a human that look at the gestalt of stories,the collection of stories, and let's say here are the clumps, the clusters that make sense.So we we do some work in our qualitative analysis to figure out the similarity and then to label it.So with affinity analysis you don't have your labels at the start.You allow the labels to emerge from the groups that are similar.Okay. Grounded theory. You had a reading on that this week.And the reason why I love this reading is because it's very it's like accessibly written, but then it goes into curiosity, creativity and surprise.And so it speaks to you as human beings with this qualitative data set.And it says if you're curious about something that means that you haven't really encountered this before, this information, this is new.And so this is useful because your own feeling of curiosity shows that it's relevant to the research question and that it's new.And so it's about sort of acknowledging ourselves as humans and that our reactions, uh, our intuitions guide the process.So curiosity tells you it's new and relevant. Uh, surprise also tells you it's unexpected.And so maybe you had an idea of, like, how people travel or figure out travel.And then somebody surprises you with their story, and then you're like, oh, this was not in my sort of theory of how people do things.Hence this is important. And so your own feeling of surprise is a useful part of the process.And then creativity again, you as a human are putting a lot of creativity into this process by asking follow up questions.Grouping things together and figuring out like what is the best way to do this?What is the best way to tell the story of like, a messy qualitative dataset?And so I'm going to quickly take you through a little example of grounded theory just so that you have,um, something some, some sort of practical example to look at.And so there's the field notes like you've taken on your, um, little sticky notes and then you're discovering or you're and labelling the variables.So again, just like affinity diagramming, you're sort of discovering what's in there and discovering relationships.And so in this story, uh, in this data sets, we're, we're seeing when you put robots into a warehouse,how are the different workers treating the robots?And so you're interviewing the workers on their interactions with the robots.And so somebody says, I kicked it before I was told not to laugh.And so they're then the researchers are saying this is an example of abusing the robot.And so they're labelling this story in that way.So you can have a reliability analysis.And that's another method of having rigour where two independent people code the same qualitative dataset.And then we look at the like statistical agreement of those codes.So that's something that you can do to show that this is a reliable scheme.So axial coding and that's finding sort of subcategories.And so we had negative treatment of the robot.And then some examples that came up in the dataset were maybe naming the robot names, impersonating the robot uh kicking the robot, abusing the robot.And so selective coding is again going deep into some sort of theoretical model of how do people work with new robots that are on the team.And so these researchers were quite interested in how conditions impacted responses to the robots.So let's say they were um, they felt like their,the quality of their workplace was going down and they had a heavy workload in, in the conditions with the robot.Would that make them treat the robot worse than if they had somehow good, more flexible conditions?Uh, and so that's, that's what they were sort of looking at.If there were some indications in the qualitative data of a kind of consequences of conditions.So this is looking into a specific selective coatings a selective subtopic.Okay. Comparative analysis. And so this is something else you can do with qualitative data.There are some points where in a high workload unit the robot is annoying.But there's maybe some sort of baseline of stress. And in the low workload unit.So they're doing sort of comparative compare comparisons of the transcripts.And then in the low workload unit it seems that the robot is found to be delightful.And then you kind of get to this theory about robots.And you can include other like qualitative concepts in other studies to feed into your theory.So again I just wanted to give you a sense it's, you know, the, the the method is called grounded theory.And so to show you how you can have some interviews and then you lead to some sort of theoretical model,this is how we can understand how robots will be treated in the workplace depending on the baseline environment, whether it's high or low workload.And so as a recap, you can do open coding, which is just figuring out what's there.Axial coding subcategories, selective coding, your little area of interest, comparing different uh conditions to then build the theory.Okay. Last kind of analysis for qualitative data somatic analysis.And so the inventor of somatic analysis is actually a University of Auckland, uh academic Jenny Braun.And I think there are like , citations to her paper on how to do somatic analysis.So like all of HCI uses this method, lots of lots of researchers around the world use the method.So basically it's where we're sort of looking at different ways to go about the same thing.We have this messy qualitative dataset. How do we make sense of it. Affinity analysis grounded uh theory and now somatic.So here we're trying to find themes. So you familiarise yourself with the data.Step one. Step two you might generate some initial codes.You might look for themes. You review the themes.You then get a finished refined name for the themes and you produce a report.Sounds pretty easy. Basically, it's again like, you know, we're kind of doing a similar thing in all this, like affinity diagramming.Grounded theory is just like, what? What are we talking about?And like, how do we summarise the themes of what we've talked about. So it's not really that complicated.Okay. So now work with another group or another peer to find  or  themes in the data from your sticky notes.Okay. I'll give you. Two minutes to come up with a theme.Some. To go. This was worst.Yes. Yes. Yes. Yes. Yes.This looks like it's going like this. Because. I don't like that.You can get, like, one thing. Yes. This is right.My. It's the same.I. Salvation stories.Well, that's just the two of us. So.Okay. So fast and furious.This is what happens. So tell me.Anybody come up with any themes? Was there a problem?Any. Any themes? Anyone? Hmm.Hmhm. There wasn't a lot of time to discuss.Um, and also not that much interview data.So you do have some challenges in finding themes? Certainly.Uh, but I am hoping that someone could sort of suggest something so that the students in theclass would have the benefit of knowing what a theme could be for this kind of data set.So we're we're looking. We're looking for a potential theme.Yes. I mean, we only kind of had two people, so it was hard to crack the thing.Yeah. One of the things we identified, it was kind of like a decision fatigue.Like, we've got so many different options. Uh, so yes, yes, we're going to hop to that.All these kind of look good catalogue. Yeah, yeah.So fantastic. So for those who didn't, who may have not heard and also for the recording, the theme that came up was decision fatigue.And so in in finding error,in trying to choose a travel place that there's just so many options available and you kind of like get exhausted about it and can't decide.So fantastic. So that's I think is is something that came up in those interviews.It might not come up in every interview, but it's certainly a very real issue, uh, with travel planning.So thank you for that. Cool.So little practice problem here.We're going to fill out these sentences. So affinity diagrams involve x and give x.Grounded theory method involves x and gives x x and y.So I'll let you have a little look at this.Try to fill in the blanks in your mind. At the end of these lectures you should be able to know the answers to this.Okay. Affinity diagrams involve.What? And give what? Any. Any suggestions?So affinity diagrams. That's the one with all the sticky notes. Pardon.Initially. Initially. Grouping. Correct.Correct. And then what does it give, uh, scope to this level?Right. Right. So affinity diagrams,you initially group things together that are similar based on your own human intuitive understanding of what similarity could be.And then you give those similarities labels. Wonderful grounded theory method involves what and gives what coding and gives the theory.That's an easy one, right? Okay. The matic analysis involves what?And gives what? So semantic analysis involves what it gives.What. Uh.Okay. So thematic analysis.Sorry. What's a suggestion?Uh, okay. So, yeah, the things on the left kind of go in the first like slot and then the things on the right.Yeah. So somatic analysis involves some coding and gives themes.Excellent, excellent. Okay. Good good good.Beautiful. So now let us go to the last part of the lecture.Okay. So the last part of the lecture just touches upon some ethical considerations.Ethical history related to human computer interaction.Okay, so Google Assistant calls a restaurant.Yeah. Oh. Uh. Let's see.I'm going to. Well, have you.I'm going to come back to that and see if we have time. So this is a older photo of, uh, some problematic some problems with, uh,automatically labelling photos that sort of come into ethical concerns around ethnicity and race.So here's another one, uh, where it's sort of adjacent to HCI because it's more around like biased training datasets,but it does come up in interaction with systems.And so part of our role in human computer interaction is to recognise ethical concerns and to try to mitigate them.So one of the uh, papers that you were reading today was value sensitive design.And that has to do with, um, a major issue around user autonomy.And so in, in the, the Google Assistant. Um, well, let's maybe get that Google Assistant, um, example up.Oh, goodness. This. I'm.Okay. Right. Sweet. Yeah.How are you? Hi. Um, I'd like to get a table for Wednesday.The seven for seven people.Um. It's for four people. For people with, um.Wednesday. Uh, again? Uh, actually, we need to go, like, I know, like, a five people.Well, you know, you can, um. How long the wait usually to, uh.Did it go up when my, uh, weekday or for next Wednesday?Uh, the seven. Oh, no. It'll come with it.You can come. Okay. Oh, I gotcha.Thanks. Bye bye. Okay, so this, uh, happened when Google was, uh, sort of presenting their assistant six years ago.So, like, early on in, in these kinds of, like, uh, sort of audio assistants in, in part of the big boom.And there was some sort of critical critiques after this came out.And so partly it has to do with whether the assistant, um, misrepresented itself.And so did it. Was it obviously an assistant? Uh, the human that was on the line?I think we could agree, like, I thought it was a human and was sort of treating it like a human.And so the value sensitive design reading brings up misrepresentation of the system and as, as something that, uh, sort of impacts user autonomy.And so if that human the who was taking a call like knew it was an assistant, would she have answered differently.Does she should she really ethically be required or like should we ethically,you know, want to know when we're interacting with real people or with assistants?Uh, I'm not sure if any of you are doing, like, job interviews these days and have to do, like, chatting.Uh, but I think it's clear with that, with those job interviews that, you know, that you're chatting with a robot or like,and you would want to know, when am I chatting with a robot or when am I chatting with an actual like interviewer?So again, these are sort of one of the major concerns around the ethical concerns around human computer interaction.Does is the system representing itself honestly or not?Then there are other examples. So system capability like do you have as a user the freedom to turn off the system or walk away from the system?Uh, some really big sort of critiques came out about Amazon warehouses working for anAmazon warehouse where you were tracked all the time and couldn't take a break,were like timed for your breaks. And so those kinds of things come up in ethical interactions with with systems.And then as humans, we change over time, learn over time.Uh, does the system have any fluidity? Does it allow?Does it sort sort of support the user over time?Uh, in in different situations and system complexity, uh, is a question that's sort of wrestled with in that article.So let's say you use some complex software.Let's take Photoshop for example. Complex enough. And so if you're a super beginner,you come to Photoshop and maybe you want to do something simple and you're not really able to do it because the system, the software is so complex.But when you've invested in learning that software, you can do a lot of things.And so there are some sort of ethical concerns about matching the system to the user expertise.If you're serving a novice, then it's ethical to give that sort of support.It's more supportive to have a simpler interface, perhaps with less control.And so that's the thing that it's not always more control is better, because sometimes more control is very complicated.And so with with less options, users may feel more in control.So anyhow, it's messy out there. And these are some of the top concerns ethical concerns in human computer interaction.So just with a super, super quick overview of like what are ethics?Uh, we're going to do this for the next ten minutes. So research with human participants.You should have done your modules on the ethics, uh, site.And so we across universities all over the world have very similar requirements.And the reasons for this come to human history.And so one big point in human history was World War two, where, uh,Nazis in Germany experimented on Jews in concentration camps, of course, without their consent.And after this happened, basically the world came together to be like, this is absolutely outrageous and tragic,and we should have a code of ethics for human research where we would never acceptany research findings that were found under these conditions without consent.And so, basically, informed consent, right to withdraw, do no harm.Risks must outweigh the benefits. They all come from World War two.Uh, the Nuremberg Code from the Nuremberg trials, another big, uh,event that led to our current code of ethics that we use all over the world is the Tuskegee syphilis study.And so a researcher had a scientific question.I want to understand how syphilis affects a human body over a life course.And so that was their research question. And they thought this, you know, this is important to know.And, uh, hence I will not give people treatment, uh, for Tuskegee for, for syphilis when it became available.And so this also got a lot of public outcry as being completely unethical.And hence we now have ethical constraints that sort of do not allow things like this to happen, where especially,uh, when there's like low or vulnerable populations that are not treated as well as they could be.So whatever sort of scientific benefit that you would get from getting these certain answers does not away outweigh the cost of human life.And so the Belmont Report comes from those particular historical events.And so it's about respect for people and respect for human life, human autonomy to allow for voluntary informed consent, uh, do no harm.So a sort of definition of what harm means. So to say that,like these human lives are not like or a human health is not so worthless that your scientific endeavour is more worthwhile than these human lives.Uh, and also a sense of justice that there's, like, distributed costs and benefits.And it's not that people in poverty are bearing greater costs than other people.So, yeah. Human ethics in New Zealand follows these same like international guidelines.But there are, um, some specific things that are quite different in New Zealand.And so the, uh, we'll, we'll get to that as well.So, so basically, I think all of this is starting to be sort of clear.Voluntary participation, informed consent, knowing exactly what procedures and data will be collected.A right to privacy.Uh, hopefully you now know the difference between the definitions of confidentiality and anonymity and then, uh, ownership of data and.Right, right to withdraw. So, in fact, in New Zealand, you can withdraw your data afterwards.And so it's a sort of special, uh, ethical case in New Zealand that often in research around the world,if you've given data, you know, the researcher gets to keep it.Whereas in New Zealand,usually participants have a couple of weeks to think about it and then have the right to withdraw their their data should they choose.Okay, so at University of Auckland it's the human participant ethics committee.Um, so this has to do with all research. Uh, and so you have examples in the module of a consent form and participant information sheet.So if you in this, in this course you're doing a mini project with a pilot study.So we're not actually getting ethics to conduct a real research study.But if you were wanting to conduct a real research study as part of an honours project or master's project or PG project,you would need to gain permission from this organising body where they look at your protocol,your proposed protocol, and then they either give you your permission to run the study or not.So it's a very important process, and you're not allowed to do research without getting permission.Okay, so now I'll just do like a few quick examples of ethics in HCI.So one is the theorem . And so that was a radiation therapy machine that had a race condition where if you input a certain, uh,strength of a laser and then like went ahead and deleted it or changed it elsewhere because of the way it was coded,it would keep that first, uh, indication.And this led to several people dying. So it's an important example of how, uh, race conditions in an interface design,the fact that you could sort of change the laser strength in a few places led to very tragic consequences.So hence, uh, having new innovation in the medical medical establishment is very slow.Things have to be tested very thoroughly to make sure that they do no harm.So another historical example is, um, the case of eliminating a hardware switch.So eliminating an on off switch and then users.Uh, so this was a sort of like Tesla telecommunications job.And so somebody like, would sit in their office and sort of take calls and for price reasons,they took out the on off switch to just make it simpler and cheaper to create.And hence the people in those jobs were not able to, like, turn off the machine so that they wouldn't be recorded.So in any case, it's people like down the line in some sort of manufacturing.We're looking at cost.And they weren't thinking about the ethical implications of that cost and how the on off switch, the presence of which would affect the end user.So it's something that we, we sort of come about to, to think about ethics at all parts of the design.And so privacy by design. So this is still something that, that we don't see where like this is a little camera here with a built in cover.And so we still don't have that. And we may argue like is that more of an ethical design.Uh, is that something that you would want to have available versus no cover?So we've talked about informed consent. Uh, other other recent example of informed consent.Uh, that was not sort is in the tech industry.And so Facebook meta conducted this experiment, uh, on emotional contagion.And so they changed people's newsfeeds based on, uh,a kind of natural language processing sentiment analysis so that people eithersaw more negative posts from friends or more positive posts from friends.And then, uh, the researchers at Facebook measured the sentiments of what people posted, and they found, uh, evidence for emotional contagion.When you see more negative posts on your feed.You are more likely to post a negative content.And so the researchers were like, oh, look at our cool study.We're going to publish it in nature. Or I think Scientific Reports, they were like very proud of their research.And then of course, there was public outcry being like, you should not manipulate people that way,like you've put millions of people in a situation where you've affected their moods and this is not ethical.And so then they like put a retraction and there was public outcry.Uh, so yeah, this is a sort of photo, sort of photo of their, of their retraction,where the Journal also expressed an apology of like allowing this, uh, project to be published.And so it's something that we're still, you know, the world is still grappling with,like tech tech companies have a lot of power with the terms of service to sort of changeour platforms at any time and to conduct as many experiments on us as they want.Uh, but this is not necessarily deemed ethical.And so the the emotional contagion experiment was one that was really noted in history as particularly unethical.So here is a practice problem that maybe I'll leave you with, uh, for another day.And we can we can talk about it, uh, as you're studying for the exam.Uh, so the basic points of this lecture is really about incorporating, like, uh, ethical considerations into the design process.And so just taking a step back all the time and thinking it's my responsibility to make sure that the user,uh, has autonomy and that the whole use of this system is ethical.And so that we all sort of share that responsibility as designers.So in this little part of lecture,what you did was you understood ethical treatment of participants and how you canapply it to your own course projects and some important historical turning points.And so now we will do our project implementation check ins.So thank you all. See you. Oh and next week, next week there's no class because it's systems week for engineers.
